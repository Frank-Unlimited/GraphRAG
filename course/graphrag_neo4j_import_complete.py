#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Microsoft GraphRAG æ•°æ®å®Œæ•´å¯¼å…¥ Neo4j å›¾æ•°æ®åº“
ä» Jupyter Notebook æå–çš„æ‰€æœ‰æœªæ³¨é‡Šä»£ç 

åŠŸèƒ½:
1. è¿æ¥ Neo4j æ•°æ®åº“
2. æ¸…ç©ºæ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
3. å¹¶è¡Œæ‰¹é‡å¯¼å…¥æ•°æ®
4. å¯¼å…¥ documents, text_units, entities, relationships, communities, community_reports
5. åˆ›å»ºèŠ‚ç‚¹å’Œå…³ç³»
6. éªŒè¯å¯¼å…¥ç»“æœ
"""

import time
import pandas as pd
import concurrent.futures
from neo4j import GraphDatabase, exceptions
from tabulate import tabulate
import os
import json


# ========== Neo4j è¿æ¥é…ç½® ==========
NEO4J_URI = "bolt://localhost:7687"  # æ³¨æ„ï¼šBoltç«¯å£æ˜¯7687ï¼Œä¸æ˜¯7474
NEO4J_USERNAME = "neo4j"
NEO4J_PASSWORD = "Han9510!"  # æ›¿æ¢ä¸ºä½ çš„å¯†ç 
NEO4J_DATABASE = "neo4j"
RETRY_TIMES = 3
RETRY_DELAY = 5

# å…¨å±€é©±åŠ¨å®ä¾‹
driver = None


# ========== è¿æ¥å‡½æ•° ==========
def connect_neo4j():
    """åˆ›å»ºNeo4jé©±åŠ¨å¹¶éªŒè¯è¿æ¥ï¼ˆå¸¦é‡è¯•ï¼‰"""
    global driver
    for i in range(RETRY_TIMES):
        try:
            if NEO4J_PASSWORD.strip() == "":
                driver = GraphDatabase.driver(NEO4J_URI, connection_timeout=30)
            else:
                driver = GraphDatabase.driver(
                    NEO4J_URI,
                    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),
                    connection_timeout=30
                )
            driver.verify_connectivity()
            print(f"âœ… ç¬¬{i+1}æ¬¡å°è¯•ï¼šNeo4jè¿æ¥æˆåŠŸï¼")
            return driver
        except exceptions.AuthError:
            print(f"âŒ ç¬¬{i+1}æ¬¡å°è¯•ï¼šç”¨æˆ·å/å¯†ç é”™è¯¯ï¼")
            break
        except exceptions.ServiceUnavailable as e:
            print(f"âŒ ç¬¬{i+1}æ¬¡å°è¯•ï¼šNeo4jæœåŠ¡æœªå¯åŠ¨æˆ–ç«¯å£é”™è¯¯ï¼é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
            if i < RETRY_TIMES - 1:
                time.sleep(RETRY_DELAY)
            else:
                break
        except Exception as e:
            print(f"âŒ ç¬¬{i+1}æ¬¡å°è¯•ï¼šè¿æ¥å¤±è´¥ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
    return None


# ========== æ¸…ç©ºæ•°æ®åº“å‡½æ•° ==========
def clear_neo4j_database():
    """æ¸…ç©ºNeo4jæ•°æ®åº“çš„æ‰€æœ‰æ•°æ®"""
    global driver
    driver = connect_neo4j()
    if not driver:
        print("âŒ Neo4jè¿æ¥å¤±è´¥ï¼Œç»ˆæ­¢æ¸…ç©ºæ“ä½œï¼")
        return

    try:
        with driver.session() as session:
            # æ­¥éª¤1ï¼šåˆ é™¤æ‰€æœ‰çº¦æŸ
            constraints = session.run("""
                SHOW CONSTRAINTS YIELD name, type, entityType, labelsOrTypes, properties
                WHERE name IS NOT NULL
            """).data()
            
            if constraints:
                print(f"ğŸ“Œ å‘ç°{len(constraints)}ä¸ªçº¦æŸï¼Œå¼€å§‹åˆ é™¤...")
                cons_count = 0
                for cons in constraints:
                    cons_name = cons['name']
                    try:
                        session.run(f"DROP CONSTRAINT {cons_name}")
                        print(f"âœ… å·²åˆ é™¤çº¦æŸï¼š{cons_name}")
                        cons_count += 1
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤çº¦æŸ{cons_name}å¤±è´¥ï¼š{str(e)}")
                print(f"ğŸ“Œ çº¦æŸåˆ é™¤å®Œæˆï¼šæˆåŠŸ{cons_count}ä¸ª")
            else:
                print("ğŸ“Œ æœªå‘ç°ä»»ä½•çº¦æŸ")

            # æ­¥éª¤2ï¼šåˆ é™¤æ‰€æœ‰ç‹¬ç«‹ç´¢å¼•
            indexes = session.run("""
                SHOW INDEXES YIELD name, type
                WHERE name IS NOT NULL AND type <> 'CONSTRAINT'
            """).data()
            
            if indexes:
                print(f"\nğŸ“Œ å‘ç°{len(indexes)}ä¸ªç´¢å¼•ï¼Œå¼€å§‹åˆ é™¤...")
                idx_count = 0
                for idx in indexes:
                    idx_name = idx['name']
                    try:
                        session.run(f"DROP INDEX {idx_name}")
                        print(f"âœ… å·²åˆ é™¤ç´¢å¼•ï¼š{idx_name}")
                        idx_count += 1
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç´¢å¼•{idx_name}å¤±è´¥ï¼š{str(e)}")
                print(f"ğŸ“Œ ç´¢å¼•åˆ é™¤å®Œæˆï¼šæˆåŠŸ{idx_count}ä¸ª")
            else:
                print("ğŸ“Œ æœªå‘ç°ä»»ä½•ç‹¬ç«‹ç´¢å¼•")

            # æ­¥éª¤3ï¼šåˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            result = session.run("MATCH (n) DETACH DELETE n")
            counters = result.consume().counters
            print(f"\nğŸ“Œ æ•°æ®åˆ é™¤ç»“æœï¼š")
            print(f"   - å·²åˆ é™¤èŠ‚ç‚¹ï¼š{counters.nodes_deleted}")
            print(f"   - å·²åˆ é™¤å…³ç³»ï¼š{counters.relationships_deleted}")

        print("\nğŸ‰ Neo4jæ•°æ®åº“å·²å®Œå…¨æ¸…ç©ºï¼")

    except Exception as e:
        print(f"\nâŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥ï¼š{str(e)}")
    finally:
        if driver:
            driver.close()


# ========== è¯»å– Parquet æ–‡ä»¶å‡½æ•° ==========
def find_and_read_parquet(filename):
    """è‡ªåŠ¨æŸ¥æ‰¾å¹¶è¯»å– parquet æ–‡ä»¶"""
    possible_paths = [
        f'../data/output/{filename}',
        f'data/output/{filename}',
        f'../../data/output/{filename}',
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {path}")
            return pd.read_parquet(path)
    
    raise FileNotFoundError(f"æ‰¾ä¸åˆ° {filename}")


# ========== å¹¶è¡Œæ‰¹é‡å¯¼å…¥å‡½æ•° ==========
def parallel_batched_import(statement, df, batch_size=100, max_workers=8):
    """ä½¿ç”¨å¹¶è¡Œå¤„ç†è¿›è¡Œæ‰¹é‡å¯¼å…¥æ•°æ®åˆ°Neo4j"""
    global driver
    
    total = len(df)
    batches = (total + batch_size - 1) // batch_size
    start_time = time.time()
    results = []
    
    print(f"å¼€å§‹å¹¶è¡Œå¯¼å…¥ {total} è¡Œæ•°æ®ï¼Œåˆ†ä¸º {batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} æ¡")
    
    def process_batch(batch_idx):
        start = batch_idx * batch_size
        end = min(start + batch_size, total)
        batch = df.iloc[start:end]
        batch_start_time = time.time()
        
        try:
            with driver.session(database=NEO4J_DATABASE) as session:
                result = session.run(
                    "UNWIND $rows AS value " + statement,   
                    rows=batch.to_dict("records")
                )
                summary = result.consume()
                batch_duration = time.time() - batch_start_time
                
                return {
                    "batch": batch_idx,
                    "rows": end - start,
                    "success": True,
                    "duration": batch_duration,
                    "counters": summary.counters
                }
        except Exception as e:
            batch_duration = time.time() - batch_start_time
            print(f"æ‰¹æ¬¡ {batch_idx} (è¡Œ {start}-{end-1}) å¤„ç†å¤±è´¥: {str(e)}")
            
            return {
                "batch": batch_idx,
                "rows": end - start,
                "success": False,
                "duration": batch_duration,
                "error": str(e)
            }
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch, i) for i in range(batches)]
        
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            result = future.result()
            results.append(result)
            
            if result["success"]:
                print(f"æ‰¹æ¬¡ {result['batch']} å®Œæˆ: {result['rows']} è¡Œ, è€—æ—¶ {result['duration']:.2f}ç§’")
            
            print(f"è¿›åº¦: {i+1}/{batches} æ‰¹æ¬¡å®Œæˆ ({((i+1)/batches*100):.1f}%)")
    
    successful_rows = sum(r["rows"] for r in results if r["success"])
    failed_rows = sum(r["rows"] for r in results if not r["success"])
    
    duration = time.time() - start_time
    rows_per_second = successful_rows / duration if duration > 0 else 0
    
    print(f"å¯¼å…¥å®Œæˆ: æ€»è®¡ {total} è¡Œ, æˆåŠŸ {successful_rows} è¡Œ, å¤±è´¥ {failed_rows} è¡Œ")
    print(f"æ€»è€—æ—¶: {duration:.2f}ç§’, å¹³å‡é€Ÿåº¦: {rows_per_second:.2f} è¡Œ/ç§’")
    
    return {
        "total_rows": total,
        "successful_rows": successful_rows,
        "failed_rows": failed_rows,
        "duration_seconds": duration,
        "rows_per_second": rows_per_second,
        "batch_results": results
    }


# ========== å¯¼å…¥æ–‡æ¡£ ==========
def create_document_nodes(df_documents):
    """åˆ›å»ºDocumentèŠ‚ç‚¹"""
    global driver
    with driver.session(database=NEO4J_DATABASE) as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:__Document__) REQUIRE d.id IS UNIQUE")
        except Exception as e:
            print(f"åˆ›å»ºçº¦æŸæ—¶å‡ºé”™ (å¯èƒ½å·²å­˜åœ¨): {e}")
    
    cypher_statement = """
    MERGE (d:__Document__ {id: value.id})
    ON CREATE SET 
        d.human_readable_id = value.human_readable_id,
        d.title = value.title,
        d.text = value.text,
        d.creation_date = value.creation_date,
        d.import_timestamp = timestamp()
    """
    
    return parallel_batched_import(cypher_statement, df_documents)


# ========== å¯¼å…¥æ–‡æœ¬å•å…ƒ ==========
def setup_chunk_constraints():
    """åˆ›å»ºChunkæ ‡ç­¾çš„çº¦æŸ"""
    global driver
    with driver.session(database=NEO4J_DATABASE) as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:__Chunk__) REQUIRE c.id IS UNIQUE")
            print("å·²åˆ›å»ºChunk.idå”¯ä¸€æ€§çº¦æŸ")
        except Exception as e:
            print(f"åˆ›å»º__Chunk__çº¦æŸæ—¶å‡ºé”™ (å¯èƒ½å·²å­˜åœ¨): {e}")


def import_chunks(df_chunks, batch_size=100, max_workers=8):
    """å¯¼å…¥æ–‡æ¡£å—(Chunk)åˆ°Neo4j"""
    global driver
    
    setup_chunk_constraints()
    
    print("å¼€å§‹å¯¼å…¥ChunkèŠ‚ç‚¹...")
    chunk_statement = """
    MERGE (c:__Chunk__ {id: value.id})
    SET c.text = value.text,
        c.n_tokens = value.n_tokens,
        c.human_readable_id = value.human_readable_id,
        c.name = value.human_readable_id
    """
    
    chunk_result = parallel_batched_import(chunk_statement, df_chunks, batch_size, max_workers)
    
    print("å‡†å¤‡Chunk-Documentå…³ç³»æ•°æ®...")
    relations_data = []
    
    for idx, row in df_chunks.iterrows():
        chunk_id = row['id']
        doc_ids_container = row['document_ids']
        
        flat_doc_ids = []
        if isinstance(doc_ids_container, list):
            for item in doc_ids_container:
                if hasattr(item, 'dtype') and hasattr(item, 'tolist'):
                    flat_doc_ids.extend(item.tolist())
                elif isinstance(item, list):
                    flat_doc_ids.extend(item)
                else:
                    flat_doc_ids.append(item)
        elif doc_ids_container is not None:
            flat_doc_ids.append(doc_ids_container)
        
        for doc_id in flat_doc_ids:
            if doc_id is not None and str(doc_id).strip() != '':
                doc_id_str = str(doc_id).strip()
                if not (doc_id_str.startswith('<elementId>') or doc_id_str.startswith('<id>')):
                    relations_data.append({
                        'chunk_id': chunk_id,
                        'document_id': doc_id_str
                    })
    
    if relations_data:
        print(f"å¼€å§‹åˆ›å»º {len(relations_data)} ä¸ªChunk-Documentå…³ç³»...")
        df_relations = pd.DataFrame(relations_data)
        
        relation_statement = """
        MATCH (c:__Chunk__ {id: value.chunk_id})
        MATCH (d:__Document__ {id: value.document_id})
        MERGE (c)-[:PART_OF]->(d)
        """
        
        relation_result = parallel_batched_import(relation_statement, df_relations, batch_size, max_workers)
        print(f"å·²åˆ›å»º {relation_result['successful_rows']} ä¸ªChunk-Documentå…³ç³»")
    
    return chunk_result


# ========== å¯¼å…¥å®ä½“ ==========
def setup_entity_constraints():
    """åˆ›å»ºEntityæ ‡ç­¾çš„çº¦æŸ"""
    global driver
    with driver.session(database=NEO4J_DATABASE) as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE e.id IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE e.name IS UNIQUE")
            print("å·²åˆ›å»º__Entity__.idå”¯ä¸€æ€§çº¦æŸ")
        except Exception as e:
            print(f"åˆ›å»º__Entity__çº¦æŸæ—¶å‡ºé”™ (å¯èƒ½å·²å­˜åœ¨): {e}")


def import_entities(df_entities, batch_size=100, max_workers=8):
    """å¯¼å…¥å®ä½“(Entity)åˆ°Neo4j"""
    global driver
    
    setup_entity_constraints()
    
    print("é¢„å¤„ç†text_unit_ids...")
    df_entities = df_entities.copy()
    
    for idx, row in df_entities.iterrows():
        text_unit_ids = row.get('text_unit_ids')
        
        if not isinstance(text_unit_ids, list):
            if isinstance(text_unit_ids, str):
                try:
                    text_unit_ids = json.loads(text_unit_ids)
                except:
                    text_unit_ids = [text_unit_ids]
            elif hasattr(text_unit_ids, 'dtype') and hasattr(text_unit_ids, 'tolist'):
                text_unit_ids = text_unit_ids.tolist()
            else:
                text_unit_ids = [text_unit_ids] if text_unit_ids is not None else []
        
        flat_text_unit_ids = []
        for item in text_unit_ids:
            if isinstance(item, list) or (hasattr(item, 'dtype') and hasattr(item, 'tolist')):
                if hasattr(item, 'tolist'):
                    flat_text_unit_ids.extend(item.tolist())
                else:
                    flat_text_unit_ids.extend(item)
            else:
                flat_text_unit_ids.append(item)
        
        flat_text_unit_ids = [str(id) for id in flat_text_unit_ids if id is not None and str(id).strip() != '']
        df_entities.at[idx, 'text_unit_ids'] = flat_text_unit_ids
    
    print("æ£€æŸ¥Neo4jåŠŸèƒ½æ”¯æŒ...")
    has_apoc = False
    
    try:
        with driver.session(database=NEO4J_DATABASE) as session:
            try:
                result = session.run("RETURN apoc.version() AS version")
                version = result.single()["version"]
                has_apoc = True
                print(f"APOCæ’ä»¶å·²å®‰è£…ï¼Œç‰ˆæœ¬: {version}")
            except Exception as e:
                print(f"æ£€æŸ¥APOCæ’ä»¶æ—¶å‡ºé”™ (å¯èƒ½æœªå®‰è£…): {e}")
    except Exception as e:
        print(f"æ£€æŸ¥Neo4jåŠŸèƒ½æ”¯æŒæ—¶å‡ºé”™: {e}")
    
    print("å¼€å§‹å¯¼å…¥__Entity__èŠ‚ç‚¹å¹¶åˆ›å»ºå…³ç³»...")
    
    if has_apoc:
        entity_statement = """
        MERGE (e:__Entity__ {id:value.id})
        SET e += value {.human_readable_id, .description, .frequency, .degree, .x, .y}
        SET e.name = replace(coalesce(value.title, value.human_readable_id, ''), '"', '')
        
        WITH e, value
        CALL apoc.create.addLabels(e, 
            CASE WHEN coalesce(value.type,"") = "" 
            THEN [] 
            ELSE [apoc.text.upperCamelCase(replace(value.type,'"',''))] 
            END
        ) YIELD node
        
        WITH node as e, value
        UNWIND value.text_unit_ids AS text_unit
        MATCH (c:__Chunk__ {id:text_unit})
        MERGE (c)-[:HAS_ENTITY]->(e)
        """
    else:
        entity_statement = """
        MERGE (e:__Entity__ {id:value.id})
        SET e += value {.human_readable_id, .description, .frequency, .degree, .x, .y}
        SET e.name = replace(coalesce(value.title, value.human_readable_id, ''), '"', '')
        
        WITH e, value
        UNWIND value.text_unit_ids AS text_unit
        MATCH (c:__Chunk__ {id:text_unit})
        MERGE (c)-[:HAS_ENTITY]->(e)
        """
    
    entity_result = parallel_batched_import(entity_statement, df_entities, batch_size, max_workers)
    
    with driver.session(database=NEO4J_DATABASE) as session:
        result = session.run("MATCH (e:__Entity__) RETURN count(e) as count")
        entity_count = result.single()["count"]
        
        result = session.run("MATCH (c:__Chunk__)-[r:HAS_ENTITY]->(e:__Entity__) RETURN count(r) as count")
        relation_count = result.single()["count"]
        
        print(f"éªŒè¯ç»“æœ: {entity_count} ä¸ª__Entity__èŠ‚ç‚¹, {relation_count} ä¸ªHAS_ENTITYå…³ç³»")
    
    return entity_result


# ========== å¯¼å…¥å…³ç³» ==========
def setup_relationship_constraints():
    """åˆ›å»ºRelationshipæ ‡ç­¾çš„çº¦æŸ"""
    global driver
    with driver.session(database=NEO4J_DATABASE) as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:__Relationship__) REQUIRE r.id IS UNIQUE")
            print("å·²åˆ›å»º__Relationship__.idå”¯ä¸€æ€§çº¦æŸ")
        except Exception as e:
            print(f"åˆ›å»º__Relationship__çº¦æŸæ—¶å‡ºé”™ (å¯èƒ½å·²å­˜åœ¨): {e}")


def import_relationships(df_relationships, batch_size=100, max_workers=8):
    """å¯¼å…¥å…³ç³»æ•°æ®åˆ°Neo4j"""
    global driver
    
    setup_relationship_constraints()
    
    print("é¢„å¤„ç†text_unit_ids...")
    df_relationships = df_relationships.copy()
    
    for idx, row in df_relationships.iterrows():
        text_unit_ids = row.get('text_unit_ids')
        
        if not isinstance(text_unit_ids, list):
            if isinstance(text_unit_ids, str):
                try:
                    text_unit_ids = json.loads(text_unit_ids)
                except:
                    text_unit_ids = [text_unit_ids]
            elif hasattr(text_unit_ids, 'dtype') and hasattr(text_unit_ids, 'tolist'):
                text_unit_ids = text_unit_ids.tolist()
            else:
                text_unit_ids = [text_unit_ids] if text_unit_ids is not None else []
        
        flat_text_unit_ids = []
        for item in text_unit_ids:
            if isinstance(item, list) or (hasattr(item, 'dtype') and hasattr(item, 'tolist')):
                if hasattr(item, 'tolist'):
                    flat_text_unit_ids.extend(item.tolist())
                else:
                    flat_text_unit_ids.extend(item)
            else:
                flat_text_unit_ids.append(item)
        
        flat_text_unit_ids = [str(id) for id in flat_text_unit_ids if id is not None and str(id).strip() != '']
        df_relationships.at[idx, 'text_unit_ids'] = flat_text_unit_ids
    
    print("å¼€å§‹å¯¼å…¥å…³ç³»æ•°æ®...")
    
    relationship_statement = """
    MERGE (r:__Relationship__ {id: value.id})
    SET r.human_readable_id = value.human_readable_id,
        r.description = value.description,
        r.weight = value.weight,
        r.combined_degree = value.combined_degree,
        r.name = value.human_readable_id
    
    WITH r, value
    MERGE (source:__Entity__ {id: value.source})
    MERGE (target:__Entity__ {id: value.target})
    
    MERGE (source)-[rel:RELATED]->(target)
    SET rel.relationship_id = value.id,
        rel.description = value.description,
        rel.weight = value.weight
    
    RETURN r.id as relationship_id
    """
    
    relationship_result = parallel_batched_import(relationship_statement, df_relationships, batch_size, max_workers)
    print(f"å·²åˆ›å»º {relationship_result['successful_rows']} ä¸ª__Relationship__èŠ‚ç‚¹å’ŒRELATEDå…³ç³»")
    
    chunk_relations = []
    for _, row in df_relationships.iterrows():
        rel_id = row['id']
        for chunk_id in row['text_unit_ids']:
            chunk_relations.append({
                'relationship_id': rel_id,
                'chunk_id': chunk_id
            })
    
    if chunk_relations:
        df_chunk_relations = pd.DataFrame(chunk_relations)
        
        chunk_rel_statement = """
        MATCH (r:__Relationship__ {id: value.relationship_id})
        MATCH (c:__Chunk__ {id: value.chunk_id})
        MERGE (c)-[:HAS_RELATIONSHIP]->(r)
        """
        
        chunk_rel_result = parallel_batched_import(chunk_rel_statement, df_chunk_relations, batch_size, max_workers)
        print(f"å·²åˆ›å»º {chunk_rel_result['successful_rows']} ä¸ªChunk-Relationshipå…³ç³»")
    
    return relationship_result


# ========== å¯¼å…¥ç¤¾åŒº ==========
def setup_community_constraints():
    """åˆ›å»ºCommunityæ ‡ç­¾çš„çº¦æŸ"""
    global driver
    with driver.session(database=NEO4J_DATABASE) as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:__Community__) REQUIRE c.id IS UNIQUE")
            print("å·²åˆ›å»º__Community__.idå”¯ä¸€æ€§çº¦æŸ")
        except Exception as e:
            print(f"åˆ›å»º__Community__çº¦æŸæ—¶å‡ºé”™ (å¯èƒ½å·²å­˜åœ¨): {e}")


def import_communities(df_communities, batch_size=100, max_workers=8):
    """å¯¼å…¥ç¤¾åŒº(Community)æ•°æ®åˆ°Neo4j"""
    global driver
    
    setup_community_constraints()
    
    print("é¢„å¤„ç†åˆ—è¡¨å­—æ®µ...")
    df_communities = df_communities.copy()
    
    list_fields = ['children', 'entity_ids', 'relationship_ids', 'text_unit_ids']
    
    for field in list_fields:
        if field in df_communities.columns:
            for idx, row in df_communities.iterrows():
                field_value = row.get(field)
                
                if not isinstance(field_value, list):
                    if isinstance(field_value, str):
                        try:
                            field_value = json.loads(field_value)
                        except:
                            field_value = [field_value]
                    elif hasattr(field_value, 'dtype') and hasattr(field_value, 'tolist'):
                        field_value = field_value.tolist()
                    else:
                        field_value = [field_value] if field_value is not None else []
                
                flat_field_value = []
                for item in field_value:
                    if isinstance(item, list) or (hasattr(item, 'dtype') and hasattr(item, 'tolist')):
                        if hasattr(item, 'tolist'):
                            flat_field_value.extend(item.tolist())
                        else:
                            flat_field_value.extend(item)
                    else:
                        flat_field_value.append(item)
                
                if field in ['entity_ids', 'relationship_ids', 'text_unit_ids']:
                    flat_field_value = [str(id) for id in flat_field_value if id is not None and str(id).strip() != '']
                
                df_communities.at[idx, field] = flat_field_value
    
    print("å¼€å§‹å¯¼å…¥ç¤¾åŒºèŠ‚ç‚¹...")
    
    community_statement = """
    MERGE (c:__Community__ {id: value.id})
    SET c.human_readable_id = value.human_readable_id,
        c.community = value.community,
        c.level = value.level,
        c.parent = value.parent,
        c.children = value.children,
        c.title = value.title,
        c.period = value.period,
        c.size = value.size,
        c.name = coalesce(value.title, value.human_readable_id, 'Community_' + value.id)
    
    RETURN c.id as community_id
    """
    
    community_result = parallel_batched_import(community_statement, df_communities, batch_size, max_workers)
    print(f"å·²åˆ›å»º {community_result['successful_rows']} ä¸ª__Community__èŠ‚ç‚¹")
    
    print("å¼€å§‹åˆ›å»ºç¤¾åŒºä¸å®ä½“çš„å…³ç³»...")
    entity_relations = []
    for _, row in df_communities.iterrows():
        community_id = row['id']
        entity_ids = row.get('entity_ids', [])
        
        for entity_id in entity_ids:
            entity_relations.append({
                'community_id': community_id,
                'entity_id': entity_id
            })
    
    if entity_relations:
        df_entity_relations = pd.DataFrame(entity_relations)
        
        entity_rel_statement = """
        MATCH (c:__Community__ {id: value.community_id})
        MATCH (e:__Entity__ {id: value.entity_id})
        MERGE (e)-[:IN_COMMUNITY]->(c)
        """
        
        entity_rel_result = parallel_batched_import(entity_rel_statement, df_entity_relations, batch_size, max_workers)
        print(f"å·²åˆ›å»º {entity_rel_result['successful_rows']} ä¸ªEntity-Communityå…³ç³»")
    
    print("å¼€å§‹åˆ›å»ºç¤¾åŒºä¸å…³ç³»çš„å…³ç³»...")
    rel_relations = []
    for _, row in df_communities.iterrows():
        community_id = row['id']
        relationship_ids = row.get('relationship_ids', [])
        
        for rel_id in relationship_ids:
            rel_relations.append({
                'community_id': community_id,
                'relationship_id': rel_id
            })
    
    if rel_relations:
        df_rel_relations = pd.DataFrame(rel_relations)
        
        rel_rel_statement = """
        MATCH (c:__Community__ {id: value.community_id})
        MATCH (r:__Relationship__ {id: value.relationship_id})
        MERGE (r)-[:IN_COMMUNITY]->(c)
        """
        
        rel_rel_result = parallel_batched_import(rel_rel_statement, df_rel_relations, batch_size, max_workers)
        print(f"å·²åˆ›å»º {rel_rel_result['successful_rows']} ä¸ªRelationship-Communityå…³ç³»")
    
    return community_result


# ========== å¯¼å…¥ç¤¾åŒºæŠ¥å‘Š ==========
def import_community_reports(df_reports, batch_size=20, max_workers=2):
    """å¯¼å…¥ç¤¾åŒºæŠ¥å‘Šæ•°æ®åˆ°Neo4j"""
    global driver
    
    print("é¢„å¤„ç†ç¤¾åŒºæŠ¥å‘Šæ•°æ®...")
    df_reports = df_reports.copy()
    
    df_reports['community_str'] = None
    df_reports['processed_findings'] = None
    
    for idx, row in df_reports.iterrows():
        if 'community' in row:
            community_str = str(row['community'])
            df_reports.at[idx, 'community_str'] = community_str
        
        findings = row.get('findings')
        
        if hasattr(findings, 'dtype') and hasattr(findings, 'tolist'):
            try:
                findings = findings.tolist()
            except Exception as e:
                findings = []
        elif not isinstance(findings, list):
            if isinstance(findings, str):
                try:
                    findings = json.loads(findings)
                except Exception as e:
                    findings = []
            else:
                findings = []
        
        if not isinstance(findings, list):
            findings = []
        
        valid_findings = []
        for i, finding in enumerate(findings):
            if isinstance(finding, dict):
                if 'summary' not in finding:
                    finding['summary'] = f"Finding_{i}"
                if 'explanation' not in finding:
                    finding['explanation'] = ""
                valid_findings.append(finding)
        
        df_reports.at[idx, 'processed_findings'] = valid_findings
    
    print("å‡†å¤‡Findingæ•°æ®...")
    findings_data = []
    
    for idx, row in df_reports.iterrows():
        community_str = row['community_str']
        processed_findings = row['processed_findings']
        
        if not isinstance(processed_findings, list):
            continue
            
        for i, finding in enumerate(processed_findings):
            if isinstance(finding, dict):
                finding_id = f"{community_str}_{i}"
                findings_data.append({
                    'finding_id': finding_id,
                    'community_id': community_str,
                    'summary': finding.get('summary', f"Finding_{i}"),
                    'explanation': finding.get('explanation', "")
                })
    
    print(f"å‡†å¤‡äº† {len(findings_data)} ä¸ªFindingæ•°æ®")
    
    print("æ­¥éª¤1: å¯¼å…¥ç¤¾åŒºèŠ‚ç‚¹...")
    
    community_statement = """
    MERGE (c:__Community__ {community: value.community_str})
    SET c.level = value.level,
        c.title = value.title,
        c.rank = value.rank,
        c.rating_explanation = value.rating_explanation,
        c.full_content = value.full_content,
        c.summary = value.summary,
        c.name = coalesce(value.title, 'Community_' + value.community_str)
    RETURN c.community as community_id
    """
    
    community_result = parallel_batched_import(community_statement, df_reports, batch_size, max_workers)
    print(f"å·²åˆ›å»º/æ›´æ–° {community_result['successful_rows']} ä¸ªç¤¾åŒºèŠ‚ç‚¹")
    
    if findings_data:
        print("æ­¥éª¤2: å¯¼å…¥FindingèŠ‚ç‚¹å’Œå…³ç³»...")
        df_findings = pd.DataFrame(findings_data)
        
        finding_statement = """
        MERGE (f:__Finding__ {id: value.finding_id})
        SET f.summary = value.summary,
            f.explanation = value.explanation,
            f.name = value.summary
        
        WITH f, value
        MATCH (c:__Community__ {community: value.community_id})
        MERGE (c)-[:HAS_FINDING]->(f)
        """
        
        finding_result = parallel_batched_import(finding_statement, df_findings, batch_size, max_workers)
        print(f"å·²åˆ›å»º {finding_result['successful_rows']} ä¸ªFindingèŠ‚ç‚¹å’ŒHAS_FINDINGå…³ç³»")
    
    return community_result


# ========== ä¸»å‡½æ•° ==========
def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å¯¼å…¥æµç¨‹"""
    global driver
    
    print("=" * 80)
    print("Microsoft GraphRAG æ•°æ®å¯¼å…¥ Neo4j å›¾æ•°æ®åº“")
    print("=" * 80)
    
    # 1. è¿æ¥æ•°æ®åº“
    print("\næ­¥éª¤1: è¿æ¥Neo4jæ•°æ®åº“...")
    driver = connect_neo4j()
    if not driver:
        print("âŒ æ— æ³•è¿æ¥åˆ°Neo4jæ•°æ®åº“ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 2. è¯»å–æ•°æ®æ–‡ä»¶
    print("\næ­¥éª¤2: è¯»å–Parquetæ•°æ®æ–‡ä»¶...")
    try:
        df_documents = find_and_read_parquet('documents.parquet')
        df_text_units = find_and_read_parquet('text_units.parquet')
        df_entities = find_and_read_parquet('entities.parquet')
        df_relations = find_and_read_parquet('relationships.parquet')
        df_communities = find_and_read_parquet('communities.parquet')
        df_communities_reports = find_and_read_parquet('community_reports.parquet')
        
        print(f"âœ… æˆåŠŸè¯»å–æ‰€æœ‰æ•°æ®æ–‡ä»¶")
        print(f"   - Documents: {len(df_documents)} è¡Œ")
        print(f"   - Text Units: {len(df_text_units)} è¡Œ")
        print(f"   - Entities: {len(df_entities)} è¡Œ")
        print(f"   - Relationships: {len(df_relations)} è¡Œ")
        print(f"   - Communities: {len(df_communities)} è¡Œ")
        print(f"   - Community Reports: {len(df_communities_reports)} è¡Œ")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # 3. å¯¼å…¥æ•°æ®
    print("\næ­¥éª¤3: å¼€å§‹å¯¼å…¥æ•°æ®åˆ°Neo4j...")
    
    try:
        print("\n3.1 å¯¼å…¥Documents...")
        create_document_nodes(df_documents)
        
        print("\n3.2 å¯¼å…¥Text Units (Chunks)...")
        import_chunks(df_text_units)
        
        print("\n3.3 å¯¼å…¥Entities...")
        import_entities(df_entities)
        
        print("\n3.4 å¯¼å…¥Relationships...")
        import_relationships(df_relations)
        
        print("\n3.5 å¯¼å…¥Communities...")
        import_communities(df_communities)
        
        print("\n3.6 å¯¼å…¥Community Reports...")
        import_community_reports(df_communities_reports)
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æ•°æ®å¯¼å…¥å®Œæˆï¼")
        print("=" * 80)
        print("\nå¯ä»¥è®¿é—® http://localhost:7474 æŸ¥çœ‹Neo4jæµè§ˆå™¨")
        print("å¸¸ç”¨æŸ¥è¯¢:")
        print("  - æŸ¥çœ‹å®ä½“å…³ç³»: MATCH path = (:__Entity__)-[:RELATED]->(:__Entity__) RETURN path LIMIT 200")
        print("  - æŸ¥çœ‹æ–‡æ¡£ç»“æ„: MATCH (d:__Document__)<-[:PART_OF]-(c:__Chunk__) RETURN * LIMIT 100")
        print("  - æŸ¥çœ‹ç¤¾åŒº: MATCH p=()-[r:IN_COMMUNITY]->() RETURN p LIMIT 25")
        
    except Exception as e:
        print(f"\nâŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("\næ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    # å¯é€‰ï¼šå…ˆæ¸…ç©ºæ•°æ®åº“
    # clear_neo4j_database()
    
    # æ‰§è¡Œå¯¼å…¥
    main()
