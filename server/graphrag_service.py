#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
GraphRAG Query Service
FastAPI service wrapping GraphRAG query functionality
"""

import asyncio
import logging
import os
from pathlib import Path
from typing import Optional, Any, List
import threading

from fastapi import FastAPI, HTTPException, Query, UploadFile, File, BackgroundTasks, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import pandas as pd
import shutil
import subprocess
import uuid
from datetime import datetime
import yaml

# Load environment variables from data/.env
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../data/.env"))

# Import OpenAI for LLM calls
from openai import AsyncOpenAI
import base64
import requests

# Import GraphRAG modules
import graphrag.api as api
from graphrag.config.load_config import load_config
from graphrag.callbacks.noop_query_callbacks import NoopQueryCallbacks
from graphrag.utils.storage import load_table_from_storage
from graphrag.storage.file_pipeline_storage import FilePipelineStorage
from graphrag.config.enums import IndexingMethod
from graphrag.logger.base import ProgressLogger

# ========================================
# Configuration
# ========================================

PROJECT_DIR = os.getenv("GRAPHRAG_PROJECT_DIR", "/Users/fengguihuan/Desktop/HHC/graphrag")
DATA_DIR_NAME = os.getenv("GRAPHRAG_DATA_DIR", "data")

# Access Key é…ç½®
QUERY_ACCESS_KEY = "hanhaochen"  # æŸ¥è¯¢æƒé™
UPDATE_ACCESS_KEY = "duping"     # æ›´æ–°æƒé™

# LLM é…ç½®ï¼ˆä» .env åŠ è½½ï¼‰
LLM_API_BASE = os.getenv("GRAPHRAG_API_BASE", "https://ark.cn-beijing.volces.com/api/v3")
LLM_API_KEY = os.getenv("GRAPHRAG_API_KEY", "")
LLM_MODEL_NAME = os.getenv("GRAPHRAG_MODEL_NAME", "doubao-1-5-lite-32k-250115")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆå…¼å®¹è±†åŒ… APIï¼‰
openai_client = AsyncOpenAI(
    api_key=LLM_API_KEY,
    base_url=LLM_API_BASE
)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è°ƒè¯•æ—¥å¿— - åœ¨åˆå§‹åŒ–å®¢æˆ·ç«¯ä¹‹å‰
logger.info(f"LLM Configuration:")
logger.info(f"  API Base: {LLM_API_BASE}")
logger.info(f"  API Key: {'*' * 20 if LLM_API_KEY else 'NOT SET'}")
logger.info(f"  Model: {LLM_MODEL_NAME}")

# Data cache
data_cache = {}

# ========================================
# Access Key é‰´æƒå‡½æ•°
# ========================================

def verify_query_access(access_key: Optional[str] = None) -> bool:
    """éªŒè¯æŸ¥è¯¢æƒé™"""
    if not access_key:
        return False
    return access_key == QUERY_ACCESS_KEY

def verify_update_access(access_key: Optional[str] = None) -> bool:
    """éªŒè¯æ›´æ–°æƒé™"""
    if not access_key:
        return False
    return access_key == UPDATE_ACCESS_KEY

# ========================================
# FastAPI Application
# ========================================

app = FastAPI(
    title="GraphRAG Query Service",
    description="FastAPI service for GraphRAG queries",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ========================================
# Request/Response Models
# ========================================

class QueryRequest(BaseModel):
    query: str = Field(..., description="Query text")
    query_type: str = Field(default="local", description="Query type: local, global, drift, basic")
    response_type: str = Field(default="text", description="Response type: text, json")
    community_level: int = Field(default=1, description="Community level")
    dynamic_community_selection: bool = Field(default=False, description="Enable dynamic community selection")
    access_key: Optional[str] = Field(None, description="Access key for authentication")

class QueryResponse(BaseModel):
    query: str
    response: str
    query_type: str
    context: str = ""

class IndexUpdateResponse(BaseModel):
    status: str
    message: str
    task_id: str = ""
    file_name: str = ""
    file_type: str = ""

class NLToCypherRequest(BaseModel):
    question: str = Field(..., description="Natural language question")
    neo4j_url: str = Field(..., description="Neo4j HTTP API URL")
    neo4j_user: str = Field(default="neo4j", description="Neo4j username")
    neo4j_password: str = Field(..., description="Neo4j password")
    access_key: Optional[str] = Field(None, description="Access key for authentication")

class NLToCypherResponse(BaseModel):
    question: str
    cypher: str
    results: Any
    explanation: str = ""

# ç´¢å¼•ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
index_tasks = {}

# ========================================
# NL to Cypher Functions
# ========================================

def get_neo4j_schema(neo4j_url: str, username: str, password: str) -> dict:
    """è·å– Neo4j å›¾è°±çš„ schema ä¿¡æ¯"""
    try:
        # ç¡®ä¿ URL æœ‰åè®®
        if not neo4j_url.startswith('http://') and not neo4j_url.startswith('https://'):
            neo4j_url = 'http://' + neo4j_url
        
        auth = base64.b64encode(f"{username}:{password}".encode()).decode()
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Basic {auth}',
            'Accept': 'application/json'
        }
        
        schema_info = {
            'node_labels': [],
            'relationship_types': [],
            'node_properties': {},
            'relationship_properties': {}
        }
        
        # è·å–èŠ‚ç‚¹æ ‡ç­¾
        response = requests.post(
            f"{neo4j_url}/tx/commit",
            headers=headers,
            json={
                "statements": [{
                    "statement": "CALL db.labels()",
                    "resultDataContents": ["row"]
                }]
            },
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('results') and data['results'][0].get('data'):
                schema_info['node_labels'] = [row['row'][0] for row in data['results'][0]['data']]
        
        # è·å–å…³ç³»ç±»å‹
        response = requests.post(
            f"{neo4j_url}/tx/commit",
            headers=headers,
            json={
                "statements": [{
                    "statement": "CALL db.relationshipTypes()",
                    "resultDataContents": ["row"]
                }]
            },
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('results') and data['results'][0].get('data'):
                schema_info['relationship_types'] = [row['row'][0] for row in data['results'][0]['data']]
        
        # è·å–èŠ‚ç‚¹å±æ€§ç¤ºä¾‹
        response = requests.post(
            f"{neo4j_url}/tx/commit",
            headers=headers,
            json={
                "statements": [{
                    "statement": """
                        MATCH (n)
                        WITH labels(n) AS labels, keys(n) AS props
                        UNWIND labels AS label
                        RETURN DISTINCT label, props
                        LIMIT 20
                    """,
                    "resultDataContents": ["row"]
                }]
            },
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('results') and data['results'][0].get('data'):
                for row in data['results'][0]['data']:
                    label = row['row'][0]
                    props = row['row'][1]
                    if label not in schema_info['node_properties']:
                        schema_info['node_properties'][label] = set()
                    schema_info['node_properties'][label].update(props)
        
        # è½¬æ¢ set ä¸º list
        for label in schema_info['node_properties']:
            schema_info['node_properties'][label] = list(schema_info['node_properties'][label])
        
        return schema_info
        
    except Exception as e:
        logger.error(f"Error getting Neo4j schema: {str(e)}")
        return {
            'node_labels': ['__Entity__', '__Chunk__', '__Relationship__'],
            'relationship_types': ['RELATED_TO', 'HAS_CHUNK'],
            'node_properties': {
                '__Entity__': ['name', 'description', 'type', 'rank', 'degree'],
                '__Chunk__': ['text', 'n_tokens', 'id'],
                '__Relationship__': ['description', 'source_id', 'target_id']
            },
            'relationship_properties': {}
        }

def build_cypher_prompt(question: str, schema: dict) -> str:
    """æ„å»º Cypher ç”Ÿæˆçš„ prompt"""
    
    node_labels_str = ", ".join(schema['node_labels'][:10])  # é™åˆ¶æ•°é‡
    rel_types_str = ", ".join(schema['relationship_types'][:10])
    
    # æ„å»ºèŠ‚ç‚¹å±æ€§è¯´æ˜
    node_props_str = ""
    for label, props in list(schema['node_properties'].items())[:5]:
        node_props_str += f"  - {label}: {', '.join(props[:10])}\n"
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ª Neo4j Cypher æŸ¥è¯¢ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç”Ÿæˆå¯¹åº”çš„ Cypher æŸ¥è¯¢è¯­å¥ã€‚

## å›¾è°± Schema ä¿¡æ¯ï¼š

### èŠ‚ç‚¹æ ‡ç­¾ï¼š
{node_labels_str}

### å…³ç³»ç±»å‹ï¼š
{rel_types_str}

### èŠ‚ç‚¹å±æ€§ç¤ºä¾‹ï¼š
{node_props_str}

## GraphRAG æ•°æ®æ¨¡å‹è¯´æ˜ï¼š
- **__Entity__**: å®ä½“èŠ‚ç‚¹ï¼ŒåŒ…å« name, description, type, human_readable_id ç­‰å±æ€§
  - ID å±æ€§ï¼š`id`ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰å’Œ `human_readable_id`ï¼ˆæ•°å­—ç±»å‹ï¼‰
  - æŸ¥è¯¢æ—¶å»ºè®®åŒæ—¶å°è¯•ä¸¤ç§ï¼šWHERE n.id = 'ç»†èƒ' OR n.human_readable_id = 1699
- **__Chunk__**: æ–‡æœ¬å—èŠ‚ç‚¹ï¼ŒåŒ…å« text, n_tokens, entity_ids, relationship_ids ç­‰å±æ€§
  - é€šè¿‡ MENTIONS å…³ç³»è¿æ¥åˆ°å®ä½“
  - é€šè¿‡ HAS_RELATIONSHIP å…³ç³»è¿æ¥åˆ°å…³ç³»èŠ‚ç‚¹
  - é€šè¿‡ PART_OF å…³ç³»è¿æ¥åˆ°æ–‡æ¡£
- **__Relationship__**: å…³ç³»å…ƒæ•°æ®èŠ‚ç‚¹ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯èŠ‚ç‚¹ä¸æ˜¯è¾¹ï¼‰ï¼ŒåŒ…å« description, source, target, human_readable_id ç­‰å±æ€§
- **__Document__**: æ–‡æ¡£èŠ‚ç‚¹ï¼Œè¡¨ç¤ºåŸå§‹æ–‡æ¡£
- **__Community__**: ç¤¾åŒºèŠ‚ç‚¹ï¼Œè¡¨ç¤ºå®ä½“çš„èšç±»
- å›¾æ•°æ®åº“ä¸­çš„è¾¹ï¼ˆå…³ç³»ï¼‰ç±»å‹ï¼š
  - RELATED_TO: å®ä½“ä¹‹é—´çš„å…³ç³»è¾¹
  - MENTIONS: æ–‡æœ¬å—æåˆ°å®ä½“
  - HAS_RELATIONSHIP: æ–‡æœ¬å—åŒ…å«å…³ç³»
  - PART_OF: æ–‡æœ¬å—å±äºæ–‡æ¡£
  - BELONGS_TO: å®ä½“å±äºç¤¾åŒº

## é‡è¦è§„åˆ™ï¼š
1. **åªè¿”å›ä¸€ä¸ª Cypher æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦è¿”å›å¤šä¸ªæŸ¥è¯¢**
2. **ä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—**
3. ä½¿ç”¨ LIMIT é™åˆ¶è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 100ï¼Œæœ€å¤š 200ï¼‰
4. ç¡®ä¿æŸ¥è¯¢è¯­æ³•æ­£ç¡®ï¼Œä½¿ç”¨æ ‡å‡† Cypher è¯­æ³•
5. ä¼˜å…ˆä½¿ç”¨ç´¢å¼•å­—æ®µï¼ˆå¦‚ name, id, titleï¼‰
6. å¯¹äºæ¨¡ç³ŠåŒ¹é…ä½¿ç”¨ CONTAINS è€Œä¸æ˜¯ =
7. è¿”å›å›¾æ•°æ®æ—¶ä½¿ç”¨ RETURN n, r, m æ ¼å¼ï¼ˆèŠ‚ç‚¹å’Œå…³ç³»ï¼‰
8. **æ•°å­—ç±»å‹çš„å±æ€§å€¼ä¸è¦åŠ å¼•å·**ï¼ˆä¾‹å¦‚ï¼šhuman_readable_id: 5573ï¼‰
9. **å­—ç¬¦ä¸²ç±»å‹çš„å±æ€§å€¼å¿…é¡»åŠ å¼•å·**ï¼ˆä¾‹å¦‚ï¼šname: 'ç»†èƒ'ï¼‰

## èŠ‚ç‚¹ç±»å‹è¯†åˆ«è§„åˆ™ï¼ˆéå¸¸é‡è¦ï¼ï¼‰ï¼š
10. **å¦‚æœç”¨æˆ·åªé—®"å®ä½“"**ï¼Œä»…æŸ¥è¯¢ __Entity__ æ ‡ç­¾
11. **å¦‚æœç”¨æˆ·åªé—®"æ–‡æœ¬"æˆ–"å†…å®¹"æˆ–"æ–‡æœ¬å—"**ï¼Œä»…æŸ¥è¯¢ __Chunk__ æ ‡ç­¾
12. **å¦‚æœç”¨æˆ·åªé—®"å…³ç³»"**ï¼ŒæŸ¥è¯¢ __Relationship__ èŠ‚ç‚¹
13. **å¦‚æœç”¨æˆ·åªé—®"æ–‡æ¡£"**ï¼ŒæŸ¥è¯¢ __Document__ æ ‡ç­¾
14. **å¦‚æœç”¨æˆ·åªé—®"ç¤¾åŒº"**ï¼ŒæŸ¥è¯¢ __Community__ æ ‡ç­¾

## å¤šèŠ‚ç‚¹ç±»å‹æŸ¥è¯¢è§„åˆ™ï¼ˆæœ€é‡è¦ï¼ï¼‰ï¼š
15. **å½“ç”¨æˆ·ä½¿ç”¨ä»¥ä¸‹å…³é”®è¯æ—¶ï¼Œå¿…é¡»åŒæ—¶æœç´¢ __Entity__ã€__Chunk__ å’Œ __Relationship__ ä¸‰ç§èŠ‚ç‚¹ï¼š**
    - "æ‰€æœ‰èŠ‚ç‚¹"
    - "æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹"
    - "ç›¸å…³çš„æ‰€æœ‰èŠ‚ç‚¹"
    - "ä¸Xç›¸å…³çš„èŠ‚ç‚¹"
    - "æ‰¾å‡ºæ‰€æœ‰ä¸Xç›¸å…³çš„"
    - "æŸ¥æ‰¾æ‰€æœ‰åŒ…å«Xçš„"
    
16. **å¤šèŠ‚ç‚¹ç±»å‹æŸ¥è¯¢çš„æ ‡å‡†æ¨¡å¼ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
    ```
    MATCH (n)
    WHERE (n:__Entity__ OR n:__Chunk__ OR n:__Relationship__)
      AND (
        n.name CONTAINS 'å…³é”®è¯' OR 
        n.text CONTAINS 'å…³é”®è¯' OR 
        n.description CONTAINS 'å…³é”®è¯'
      )
    OPTIONAL MATCH (n)-[r]-(m)
    RETURN n, r, m
    LIMIT 100
    ```
    
17. **æ³¨æ„äº‹é¡¹ï¼š**
    - ä¸è¦åªæŸ¥è¯¢ __Entity__ï¼Œå¿…é¡»åŒ…å«æ‰€æœ‰ä¸‰ç§èŠ‚ç‚¹ç±»å‹
    - ä½¿ç”¨ OR è¿æ¥ä¸åŒèŠ‚ç‚¹çš„å±æ€§å­—æ®µï¼ˆname, text, descriptionï¼‰
    - ä½¿ç”¨ OPTIONAL MATCH è·å–èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»
    - ç¡®ä¿ RETURN è¯­å¥åŒ…å« n, r, m ä»¥è¿”å›å®Œæ•´çš„å›¾ç»“æ„

## å¸¸è§æŸ¥è¯¢æ¨¡å¼ç¤ºä¾‹ï¼š

### å•ä¸€èŠ‚ç‚¹ç±»å‹æŸ¥è¯¢ï¼š
- ä»…æŸ¥æ‰¾å®ä½“: MATCH (n:__Entity__) WHERE n.name CONTAINS 'å…³é”®è¯' RETURN n LIMIT 50
- æ ¹æ® human_readable_id æŸ¥æ‰¾å®ä½“: MATCH (n:__Entity__) WHERE n.human_readable_id = 1699 RETURN n
- ä»…æŸ¥æ‰¾æ–‡æœ¬å—: MATCH (n:__Chunk__) WHERE n.text CONTAINS 'å…³é”®è¯' RETURN n LIMIT 50
- ä»…æŸ¥æ‰¾å…³ç³»èŠ‚ç‚¹: MATCH (r:__Relationship__) WHERE r.description CONTAINS 'å…³é”®è¯' RETURN r LIMIT 50

### å¤šèŠ‚ç‚¹ç±»å‹æŸ¥è¯¢ï¼ˆé‡ç‚¹ï¼ï¼‰ï¼š
- **æ‰¾å‡ºæ‰€æœ‰ä¸"æ»¤çº¸æ¡"ç›¸å…³çš„èŠ‚ç‚¹ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼‰**:
  ```
  MATCH (n)
  WHERE (n:__Entity__ OR n:__Chunk__ OR n:__Relationship__)
    AND (n.name CONTAINS 'æ»¤çº¸æ¡' OR n.text CONTAINS 'æ»¤çº¸æ¡' OR n.description CONTAINS 'æ»¤çº¸æ¡')
  OPTIONAL MATCH (n)-[r]-(m)
  RETURN n, r, m
  LIMIT 100
  ```

- **æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"ç»†èƒ"çš„èŠ‚ç‚¹åŠå…¶å…³ç³»ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼‰**:
  ```
  MATCH (n)
  WHERE (n:__Entity__ OR n:__Chunk__ OR n:__Relationship__)
    AND (n.name CONTAINS 'ç»†èƒ' OR n.text CONTAINS 'ç»†èƒ' OR n.description CONTAINS 'ç»†èƒ')
  OPTIONAL MATCH (n)-[r]-(m)
  RETURN n, r, m
  LIMIT 100
  ```

### ç‰¹å®šå…³ç³»æŸ¥è¯¢ï¼š
- æŸ¥æ‰¾å®ä½“åŠå…¶å›¾è¾¹å…³ç³»: MATCH (n:__Entity__)-[r:RELATED_TO]-(m:__Entity__) WHERE n.name CONTAINS 'å…³é”®è¯' RETURN n, r, m LIMIT 50
- æŸ¥æ‰¾å®ä½“åŠå…¶ç›¸å…³æ–‡æœ¬å—: MATCH (e:__Entity__)<-[:MENTIONS]-(c:__Chunk__) WHERE e.human_readable_id = 1699 RETURN e, c LIMIT 50

## ç”¨æˆ·é—®é¢˜ï¼š
{question}

## Cypher æŸ¥è¯¢ï¼š
"""
    return prompt

def fix_cypher_syntax(cypher: str) -> str:
    """ä¿®å¤å¸¸è§çš„ Cypher è¯­æ³•é”™è¯¯"""
    # ä¿®å¤ MATCH (n:Label1 OR Label2) è¿™ç§é”™è¯¯è¯­æ³•
    # æ­£ç¡®è¯­æ³•åº”è¯¥æ˜¯ MATCH (n) WHERE (n:Label1 OR n:Label2)
    import re
    
    # æŸ¥æ‰¾ MATCH (å˜é‡:æ ‡ç­¾1 OR æ ‡ç­¾2 ...) æ¨¡å¼
    pattern = r'MATCH\s+\((\w+):([^)]+)\)'
    
    def replace_match(match):
        var = match.group(1)
        labels_part = match.group(2)
        
        # å¦‚æœåŒ…å« ORï¼Œéœ€è¦é‡å†™
        if ' OR ' in labels_part.upper():
            # æå–æ‰€æœ‰æ ‡ç­¾
            labels = re.split(r'\s+OR\s+', labels_part, flags=re.IGNORECASE)
            labels = [l.strip().split(':')[-1].strip() for l in labels]
            
            # æ„å»ºæ–°çš„ WHERE å­å¥
            conditions = [f"{var}:{label}" for label in labels if label]
            where_clause = ' OR '.join(conditions)
            
            return f'MATCH ({var}) WHERE ({where_clause})'
        
        return match.group(0)
    
    cypher = re.sub(pattern, replace_match, cypher)
    
    return cypher

def fix_numeric_values(cypher: str) -> str:
    """ä¿®å¤æ•°å­—ç±»å‹è¢«é”™è¯¯åŠ å¼•å·çš„é—®é¢˜"""
    import re
    
    # å¸¸è§çš„æ•°å­—ç±»å‹å±æ€§å
    numeric_properties = [
        'id', 'human_readable_id', 'rank', 'degree', 'n_tokens', 
        'level', 'size', 'count', 'weight', 'score'
    ]
    
    # ä¸ºæ¯ä¸ªæ•°å­—å±æ€§ä¿®å¤å¼•å·é—®é¢˜
    for prop in numeric_properties:
        # åŒ¹é…æ¨¡å¼: property: 'æ•°å­—' æˆ– property: "æ•°å­—"
        # æ›¿æ¢ä¸º: property: æ•°å­—
        pattern = rf'({prop}\s*:\s*)["\'](\d+)["\']'
        cypher = re.sub(pattern, r'\1\2', cypher)
    
    return cypher

async def generate_cypher_with_llm(prompt: str) -> str:
    """è°ƒç”¨ LLM ç”Ÿæˆ Cypher æŸ¥è¯¢"""
    try:
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª Neo4j Cypher æŸ¥è¯¢ä¸“å®¶ã€‚åªè¿”å›ä¸€ä¸ª Cypher æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šã€‚æ³¨æ„ï¼šæ•°å­—ç±»å‹çš„å±æ€§å€¼ä¸è¦åŠ å¼•å·ï¼ˆå¦‚ id: 123ï¼‰ï¼Œå­—ç¬¦ä¸²ç±»å‹çš„å±æ€§å€¼å¿…é¡»åŠ å¼•å·ï¼ˆå¦‚ name: 'å¼ ä¸‰'ï¼‰ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
            max_tokens=500
        )
        
        cypher = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        cypher = cypher.replace("```cypher", "").replace("```sql", "").replace("```", "").strip()
        
        # å¦‚æœæœ‰å¤šä¸ª MATCH è¯­å¥ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´çš„æŸ¥è¯¢
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª MATCH åˆ°ç¬¬ä¸€ä¸ª LIMIT æˆ– RETURN çš„å®Œæ•´è¯­å¥
        if cypher.upper().count('MATCH') > 1:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª MATCH çš„ä½ç½®
            first_match = cypher.upper().find('MATCH')
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª LIMIT åé¢çš„ä½ç½®
            first_limit = cypher.upper().find('LIMIT', first_match)
            if first_limit != -1:
                # æ‰¾åˆ° LIMIT åé¢çš„æ•°å­—
                limit_end = first_limit + 5  # "LIMIT" é•¿åº¦
                while limit_end < len(cypher) and (cypher[limit_end].isdigit() or cypher[limit_end].isspace()):
                    limit_end += 1
                cypher = cypher[first_match:limit_end].strip()
            else:
                # å¦‚æœæ²¡æœ‰ LIMITï¼Œæ‰¾ç¬¬ä¸€ä¸ª RETURN åçš„æ¢è¡Œæˆ–ç¬¬äºŒä¸ª MATCH
                second_match = cypher.upper().find('MATCH', first_match + 5)
                if second_match != -1:
                    cypher = cypher[first_match:second_match].strip()
        
        # ç§»é™¤å¯èƒ½çš„è§£é‡Šæ–‡å­—ï¼ˆåªä¿ç•™ MATCH/CREATE/MERGE ç­‰å¼€å¤´çš„è¯­å¥ï¼‰
        lines = cypher.split('\n')
        cypher_lines = []
        for line in lines:
            line = line.strip()
            if line and (
                line.upper().startswith('MATCH') or 
                line.upper().startswith('RETURN') or
                line.upper().startswith('WHERE') or
                line.upper().startswith('WITH') or
                line.upper().startswith('LIMIT') or
                line.upper().startswith('ORDER') or
                line.upper().startswith('CREATE') or
                line.upper().startswith('MERGE') or
                line.upper().startswith('OPTIONAL') or
                line.upper().startswith('UNWIND') or
                line.upper().startswith('CALL')
            ):
                cypher_lines.append(line)
        
        if cypher_lines:
            cypher = ' '.join(cypher_lines)
        
        # ä¿®å¤å¸¸è§çš„è¯­æ³•é”™è¯¯
        cypher = fix_cypher_syntax(cypher)
        
        # ä¿®å¤æ•°å­—ç±»å‹è¢«é”™è¯¯åŠ å¼•å·çš„é—®é¢˜
        cypher = fix_numeric_values(cypher)
        
        logger.info(f"Generated Cypher: {cypher}")
        return cypher
        
    except Exception as e:
        logger.error(f"Error generating Cypher with LLM: {str(e)}")
        raise HTTPException(status_code=500, detail=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")

def execute_neo4j_cypher(cypher: str, neo4j_url: str, username: str, password: str) -> dict:
    """æ‰§è¡Œ Cypher æŸ¥è¯¢"""
    try:
        # ç¡®ä¿ URL æœ‰åè®®
        if not neo4j_url.startswith('http://') and not neo4j_url.startswith('https://'):
            neo4j_url = 'http://' + neo4j_url
        
        auth = base64.b64encode(f"{username}:{password}".encode()).decode()
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Basic {auth}',
            'Accept': 'application/json'
        }
        
        response = requests.post(
            f"{neo4j_url}/tx/commit",
            headers=headers,
            json={
                "statements": [{
                    "statement": cypher,
                    "resultDataContents": ["row", "graph"]
                }]
            },
            timeout=30
        )
        
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=f"Neo4j æŸ¥è¯¢å¤±è´¥: {response.text}")
        
        data = response.json()
        
        # æ£€æŸ¥é”™è¯¯
        if data.get('errors') and len(data['errors']) > 0:
            error_msg = data['errors'][0].get('message', 'Unknown error')
            raise HTTPException(status_code=400, detail=f"Cypher æŸ¥è¯¢é”™è¯¯: {error_msg}")
        
        return data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error executing Cypher: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰§è¡ŒæŸ¥è¯¢å¤±è´¥: {str(e)}")

# ========================================
# Data Loading
# ========================================

async def load_data():
    """Load GraphRAG data into cache"""
    global data_cache
    
    if data_cache:
        return data_cache
    
    try:
        project_path = os.path.join(PROJECT_DIR, DATA_DIR_NAME)
        logger.info(f"Loading configuration from: {project_path}")
        
        # Load config
        graphrag_config = load_config(Path(project_path))
        
        # Get output directory
        output_dir = Path(graphrag_config.output.base_dir)
        if not output_dir.is_absolute():
            output_dir = Path(project_path) / output_dir
        
        logger.info(f"Using output directory: {output_dir}")
        
        # Create storage
        storage = FilePipelineStorage(root_dir=str(output_dir))
        
        # Load data tables
        logger.info("Loading data tables...")
        entities = await load_table_from_storage("entities", storage)
        logger.info(f"Loaded {len(entities)} entities")
        
        text_units = await load_table_from_storage("text_units", storage)
        logger.info(f"Loaded {len(text_units)} text units")
        
        communities = await load_table_from_storage("communities", storage)
        logger.info(f"Loaded {len(communities)} communities")
        
        community_reports = await load_table_from_storage("community_reports", storage)
        logger.info(f"Loaded {len(community_reports)} community reports")
        
        relationships = await load_table_from_storage("relationships", storage)
        logger.info(f"Loaded {len(relationships)} relationships")
        
        # Load covariates (optional)
        try:
            covariates = await load_table_from_storage("covariates", storage)
            logger.info(f"Loaded {len(covariates)} covariates")
        except Exception:
            covariates = None
            logger.info("No covariates found")
        
        # Cache data
        data_cache = {
            "config": graphrag_config,
            "entities": entities,
            "text_units": text_units,
            "communities": communities,
            "community_reports": community_reports,
            "relationships": relationships,
            "covariates": covariates
        }
        
        logger.info("Data loading complete")
        return data_cache
        
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}", exc_info=True)
        raise

# ========================================
# Query Execution
# ========================================

async def execute_query(
    query: str,
    query_type: str = "local",
    response_type: str = "text",
    community_level: int = 1,
    dynamic_community_selection: bool = False
) -> dict:
    """Execute GraphRAG query"""
    try:
        data = await load_data()
        
        # Setup callbacks
        context_data = {}
        
        def on_context(context):
            nonlocal context_data
            context_data = context
        
        callbacks = NoopQueryCallbacks()
        callbacks.on_context = on_context
        
        logger.info(f"Executing {query_type} query: {query}")
        
        # Execute query based on type
        if query_type.lower() == "local":
            response, context = await api.local_search(
                config=data["config"],
                entities=data["entities"],
                communities=data["communities"],
                community_reports=data["community_reports"],
                text_units=data["text_units"],
                relationships=data["relationships"],
                covariates=data["covariates"],
                community_level=community_level,
                response_type=response_type,
                query=query,
                callbacks=[callbacks]
            )
        
        elif query_type.lower() == "global":
            response, context = await api.global_search(
                config=data["config"],
                entities=data["entities"],
                communities=data["communities"],
                community_reports=data["community_reports"],
                community_level=community_level,
                dynamic_community_selection=dynamic_community_selection,
                response_type=response_type,
                query=query,
                callbacks=[callbacks]
            )
        
        elif query_type.lower() == "drift":
            response, context = await api.drift_search(
                config=data["config"],
                entities=data["entities"],
                communities=data["communities"],
                community_reports=data["community_reports"],
                text_units=data["text_units"],
                relationships=data["relationships"],
                community_level=community_level,
                response_type=response_type,
                query=query,
                callbacks=[callbacks]
            )
        
        elif query_type.lower() == "basic":
            response, context = await api.basic_search(
                config=data["config"],
                text_units=data["text_units"],
                query=query,
                callbacks=[callbacks]
            )
        
        else:
            raise ValueError(f"Unsupported query type: {query_type}")
        
        logger.info("Query completed successfully")
        
        # ğŸ” DEBUG: æ‰“å°ä¸Šä¸‹æ–‡æ•°æ®ç”¨äºè°ƒè¯•
        logger.info(f"Context data type: {type(context_data)}")
        logger.info(f"Context data keys: {context_data.keys() if isinstance(context_data, dict) else 'Not a dict'}")
        if isinstance(context_data, dict):
            for key, value in context_data.items():
                logger.info(f"Context[{key}]: {type(value)} - {str(value)[:200]}")
        
        return {
            "query": query,
            "response": response,
            "query_type": query_type,
            "context": str(context_data)
        }
    
    except Exception as e:
        logger.error(f"Query execution error: {str(e)}", exc_info=True)
        raise

# ========================================
# API Endpoints
# ========================================

@app.get("/", response_class=HTMLResponse)
async def root():
    """Root endpoint - redirect to test page"""
    static_index = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(static_index):
        with open(static_index, 'r', encoding='utf-8') as f:
            return f.read()
    return """
    <html>
        <body>
            <h1>GraphRAG Query Service</h1>
            <p>Service is running. Visit <a href="/docs">/docs</a> for API documentation.</p>
        </body>
    </html>
    """

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "ok", "version": "1.0.0"}

@app.post("/api/nl-to-cypher", response_model=NLToCypherResponse)
async def nl_to_cypher(request: NLToCypherRequest):
    """
    è‡ªç„¶è¯­è¨€è½¬ Cypher æŸ¥è¯¢
    
    ä½¿ç”¨ LLM å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º Cypher æŸ¥è¯¢è¯­å¥ï¼Œå¹¶æ‰§è¡ŒæŸ¥è¯¢è¿”å›ç»“æœã€‚
    
    Parameters:
    - question: è‡ªç„¶è¯­è¨€é—®é¢˜
    - neo4j_url: Neo4j HTTP API URL
    - neo4j_user: Neo4j ç”¨æˆ·å
    - neo4j_password: Neo4j å¯†ç 
    - access_key: è®¿é—®å¯†é’¥ï¼ˆéœ€è¦æŸ¥è¯¢æƒé™ï¼‰
    
    Returns:
    - NLToCypherResponse åŒ…å«ç”Ÿæˆçš„ Cypherã€æŸ¥è¯¢ç»“æœå’Œè§£é‡Š
    """
    # éªŒè¯è®¿é—®æƒé™
    if not verify_query_access(request.access_key):
        logger.warning(f"Unauthorized NL to Cypher attempt with access_key: {request.access_key}")
        raise HTTPException(
            status_code=403, 
            detail="è®¿é—®è¢«æ‹’ç»ï¼šæ— æ•ˆçš„è®¿é—®å¯†é’¥ã€‚éœ€è¦æœ‰æ•ˆçš„ access_key"
        )
    
    try:
        logger.info(f"NL to Cypher request: {request.question}")
        
        # 1. è·å– Neo4j schema
        logger.info("Getting Neo4j schema...")
        schema = get_neo4j_schema(
            request.neo4j_url,
            request.neo4j_user,
            request.neo4j_password
        )
        
        # 2. æ„å»º prompt
        logger.info("Building prompt...")
        prompt = build_cypher_prompt(request.question, schema)
        
        # 3. è°ƒç”¨ LLM ç”Ÿæˆ Cypher
        logger.info("Generating Cypher with LLM...")
        cypher_query = await generate_cypher_with_llm(prompt)
        
        # 4. æ‰§è¡Œ Cypher æŸ¥è¯¢
        logger.info(f"Executing Cypher: {cypher_query}")
        results = execute_neo4j_cypher(
            cypher_query,
            request.neo4j_url,
            request.neo4j_user,
            request.neo4j_password
        )
        
        # 5. è¿”å›ç»“æœ
        return NLToCypherResponse(
            question=request.question,
            cypher=cypher_query,
            results=results,
            explanation=f"å·²å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º Cypher æŸ¥è¯¢å¹¶æ‰§è¡Œ"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"NL to Cypher error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

@app.get("/api/query", response_model=QueryResponse)
async def query_get(
    query: str = Query(..., description="Query text"),
    query_type: str = Query("local", description="Query type: local, global, drift, basic"),
    response_type: str = Query("text", description="Response type: text, json"),
    community_level: int = Query(1, description="Community level (1-3)"),
    dynamic_community_selection: bool = Query(False, description="Enable dynamic community selection"),
    access_key: Optional[str] = Query(None, description="Access key for authentication")
):
    """
    Execute GraphRAG query (GET method)
    
    Parameters:
    - query: The search query text
    - query_type: Type of search (local, global, drift, basic)
    - response_type: Format of response (text, json)
    - community_level: Community hierarchy level to search
    - dynamic_community_selection: Use dynamic community selection for global search
    - access_key: Access key for authentication (required: 'hanhaochen')
    
    Returns:
    - QueryResponse with query results and context
    """
    # éªŒè¯è®¿é—®æƒé™
    if not verify_query_access(access_key):
        logger.warning(f"Unauthorized query attempt with access_key: {access_key}")
        raise HTTPException(
            status_code=403, 
            detail="è®¿é—®è¢«æ‹’ç»ï¼šæ— æ•ˆçš„è®¿é—®å¯†é’¥ã€‚æŸ¥è¯¢éœ€è¦æœ‰æ•ˆçš„ access_key"
        )
    
    try:
        result = await execute_query(
            query=query,
            query_type=query_type,
            response_type=response_type,
            community_level=community_level,
            dynamic_community_selection=dynamic_community_selection
        )
        return JSONResponse(content=result)
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"API error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")

@app.post("/api/query", response_model=QueryResponse)
async def query_post(request: QueryRequest):
    """
    Execute GraphRAG query (POST method)
    
    Request body should contain QueryRequest model with query parameters.
    Requires access_key='hanhaochen' for authentication.
    """
    # éªŒè¯è®¿é—®æƒé™
    if not verify_query_access(request.access_key):
        logger.warning(f"Unauthorized query attempt with access_key: {request.access_key}")
        raise HTTPException(
            status_code=403, 
            detail="è®¿é—®è¢«æ‹’ç»ï¼šæ— æ•ˆçš„è®¿é—®å¯†é’¥ã€‚æŸ¥è¯¢éœ€è¦æœ‰æ•ˆçš„ access_key"
        )
    
    try:
        result = await execute_query(
            query=request.query,
            query_type=request.query_type,
            response_type=request.response_type,
            community_level=request.community_level,
            dynamic_community_selection=request.dynamic_community_selection
        )
        return JSONResponse(content=result)
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"API error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")

# ========================================
# Startup
# ========================================

@app.on_event("startup")
async def startup_event():
    """Preload data on startup"""
    logger.info("Starting GraphRAG Query Service...")
    try:
        await load_data()
        logger.info("Service ready")
    except Exception as e:  
        logger.error(f"Startup failed: {str(e)}", exc_info=True)

# ========================================
# Index Update Functions
# ========================================

def run_index_update(file_path: str, file_type: str, task_id: str):
    """Run GraphRAG index update in background using Python API"""
    task_log_file = None
    task_logger = None
    
    try:
        # åˆ›å»ºä»»åŠ¡ä¸“å±æ—¥å¿—æ–‡ä»¶
        data_dir = os.path.join(PROJECT_DIR, DATA_DIR_NAME)
        logs_dir = os.path.join(data_dir, "logs")
        os.makedirs(logs_dir, exist_ok=True)
        
        task_log_file = os.path.join(logs_dir, f"task_{task_id}.log")
        
        # åˆ›å»ºä»»åŠ¡ä¸“å±çš„ logger
        task_logger = logging.getLogger(f"task_{task_id}")
        task_logger.setLevel(logging.INFO)
        task_logger.handlers = []  # æ¸…é™¤å·²æœ‰çš„ handlers
        
        # æ·»åŠ æ–‡ä»¶ handler
        file_handler = logging.FileHandler(task_log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s', 
                                     datefmt='%Y-%m-%d %H:%M:%S')
        file_handler.setFormatter(formatter)
        task_logger.addHandler(file_handler)
        
        def log_to_file(message, level='info'):
            """å†™å…¥ä»»åŠ¡æ—¥å¿—"""
            if level == 'error':
                task_logger.error(message)
            elif level == 'warning':
                task_logger.warning(message)
            else:
                task_logger.info(message)
            logger.info(f"Task {task_id}: {message}")
        
        index_tasks[task_id]["status"] = "running"
        index_tasks[task_id]["message"] = "æ­£åœ¨æ›´æ–°ç´¢å¼•..."
        index_tasks[task_id]["log_file"] = task_log_file
        
        log_to_file("=" * 80)
        log_to_file(f"å¼€å§‹ç´¢å¼•æ›´æ–°ä»»åŠ¡")
        log_to_file(f"ä»»åŠ¡ID: {task_id}")
        log_to_file(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        log_to_file(f"æ–‡ä»¶ç±»å‹: {file_type}")
        log_to_file("=" * 80)
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªé…ç½®æ–‡ä»¶ï¼ˆç›´æ¥ä½¿ç”¨é™æ€é…ç½®ï¼‰
        if file_type == "pdf":
            config_file = "settings_pdf.yaml"
            log_to_file(f"ä½¿ç”¨ PDF é…ç½®æ–‡ä»¶: {config_file}")
        else:
            config_file = "settings.yaml"
            log_to_file(f"ä½¿ç”¨æ–‡æœ¬é…ç½®æ–‡ä»¶: {config_file}")
        
        config_path = os.path.join(data_dir, config_file)
        
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        log_to_file(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
        log_to_file("å¼€å§‹æ‰§è¡Œ GraphRAG ç´¢å¼•æ›´æ–°ï¼ˆä½¿ç”¨ Python APIï¼‰...")
        log_to_file("-" * 80)
        
        # ä½¿ç”¨ Python API è€Œä¸æ˜¯å‘½ä»¤è¡Œ
        from graphrag.config.enums import IndexingMethod
        from graphrag.logger.base import ProgressLogger
        from graphrag.logger.progress import Progress
        
        # åˆ›å»ºè‡ªå®šä¹‰è¿›åº¦è®°å½•å™¨ï¼Œå°†è¾“å‡ºå†™å…¥æ—¥å¿—æ–‡ä»¶
        class TaskProgressLogger(ProgressLogger):
            def __init__(self, log_func):
                self.log_func = log_func
                self._disposed = False
                
            def __call__(self, update: Progress):
                """å¤„ç†è¿›åº¦æ›´æ–°"""
                if update.description:
                    message = update.description
                elif update.percent is not None:
                    message = f"è¿›åº¦: {update.percent * 100:.1f}%"
                elif update.completed_items is not None and update.total_items is not None:
                    message = f"è¿›åº¦: {update.completed_items}/{update.total_items}"
                else:
                    message = str(update)
                self.log_func(f"PROGRESS: {message}")
                
            def child(self, prefix: str, transient: bool = True) -> "TaskProgressLogger":
                """åˆ›å»ºå­è®°å½•å™¨"""
                # åˆ›å»ºä¸€ä¸ªæ–°çš„å­è®°å½•å™¨ï¼Œå¸¦æœ‰å‰ç¼€
                child_logger = TaskProgressLogger(self.log_func)
                child_logger.log_func(f"å¼€å§‹å­ä»»åŠ¡: {prefix}")
                return child_logger
                
            def dispose(self):
                """æ¸…ç†èµ„æº"""
                self._disposed = True
                
            def force_refresh(self) -> None:
                """å¼ºåˆ¶åˆ·æ–°"""
                pass
                
            def stop(self) -> None:
                """åœæ­¢è®°å½•"""
                self._disposed = True
                
            def info(self, message: str) -> None:
                self.log_func(f"INFO: {message}")
                
            def error(self, message: str) -> None:
                self.log_func(f"ERROR: {message}", 'error')
                
            def warning(self, message: str) -> None:
                self.log_func(f"WARNING: {message}", 'warning')
                
            def success(self, message: str) -> None:
                self.log_func(f"SUCCESS: {message}")
        
        progress_logger = TaskProgressLogger(log_to_file)
        
        # åŠ è½½é…ç½®ï¼ˆç›´æ¥ä½¿ç”¨é™æ€é…ç½®æ–‡ä»¶ï¼‰
        from graphrag.config.load_config import load_config
        log_to_file(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        graphrag_config = load_config(Path(data_dir), Path(config_path))
        
        log_to_file("é…ç½®åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„ç´¢å¼•æ–‡ä»¶
        # ä½¿ç”¨é…ç½®ä¸­çš„è¾“å‡ºç›®å½•
        output_base_dir = graphrag_config.output.base_dir
        if not os.path.isabs(output_base_dir):
            output_dir = os.path.join(data_dir, output_base_dir)
        else:
            output_dir = output_base_dir
            
        entities_file = os.path.join(output_dir, "entities.parquet")
        has_existing_index = os.path.exists(entities_file)
        
        log_to_file(f"æ£€æŸ¥è¾“å‡ºç›®å½•: {output_dir}")
        log_to_file(f"æ£€æŸ¥ç´¢å¼•æ–‡ä»¶: {entities_file}")
        
        if has_existing_index:
            log_to_file(f"âœ“ æ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•æ–‡ä»¶")
            log_to_file("å°†æ‰§è¡Œå¢é‡æ›´æ–°ï¼ˆä»…å¤„ç†æ–°æ–‡æ¡£ï¼‰")
        else:
            log_to_file("âœ— æœªæ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•")
            log_to_file("å°†æ‰§è¡Œå®Œæ•´ç´¢å¼•æ„å»º")
            log_to_file("è­¦å‘Šï¼šé¦–æ¬¡æ„å»ºå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼")
        
        log_to_file("å¼€å§‹æ„å»ºç´¢å¼•...")
        
        # è¿è¡Œç´¢å¼•æ„å»ºï¼ˆä½¿ç”¨ asyncioï¼‰
        import asyncio
        
        async def build_index_async():
            log_to_file("è°ƒç”¨ api.build_index...")
            result = await api.build_index(
                config=graphrag_config,
                method=IndexingMethod.Standard,
                is_update_run=has_existing_index,  # åªæœ‰å­˜åœ¨ç´¢å¼•æ—¶æ‰å¢é‡æ›´æ–°
                memory_profile=False,
                progress_logger=progress_logger
            )
            return result
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            index_result = loop.run_until_complete(build_index_async())
            loop.close()
        except Exception as e:
            log_to_file(f"ç´¢å¼•æ„å»ºå¼‚å¸¸: {str(e)}", 'error')
            raise
        
        log_to_file("-" * 80)
        log_to_file("ç´¢å¼•æ„å»ºå®Œæˆï¼Œå¤„ç†ç»“æœ:")
        
        # å¤„ç†ç»“æœ
        has_error = False
        for workflow_result in index_result:
            if workflow_result.errors:
                has_error = True
                log_to_file(f"å·¥ä½œæµ [{workflow_result.workflow}] å¤±è´¥:", 'error')
                for error in workflow_result.errors:
                    log_to_file(f"  é”™è¯¯: {error}", 'error')
            else:
                log_to_file(f"å·¥ä½œæµ [{workflow_result.workflow}] æˆåŠŸ")
        
        return_code = 1 if has_error else 0
        log_to_file(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œè¿”å›ç : {return_code}")
        
        if return_code == 0:
            log_to_file("âœ… ç´¢å¼•æ›´æ–°æˆåŠŸï¼")
            index_tasks[task_id]["status"] = "completed"
            index_tasks[task_id]["message"] = "ç´¢å¼•æ›´æ–°æˆåŠŸï¼"
            
            # è¯»å–å®Œæ•´æ—¥å¿—ä½œä¸ºè¾“å‡º
            with open(task_log_file, 'r', encoding='utf-8') as f:
                index_tasks[task_id]["output"] = f.read()
            
            logger.info(f"Index update completed for task {task_id}")
            
            # æ¸…é™¤æ•°æ®ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
            global data_cache
            data_cache = {}
            log_to_file("æ•°æ®ç¼“å­˜å·²æ¸…é™¤ï¼Œä¸‹æ¬¡æŸ¥è¯¢å°†é‡æ–°åŠ è½½")
        else:
            log_to_file(f"âŒ ç´¢å¼•æ›´æ–°å¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            index_tasks[task_id]["status"] = "failed"
            index_tasks[task_id]["message"] = f"ç´¢å¼•æ›´æ–°å¤±è´¥ï¼Œè¿”å›ç : {return_code}"
            
            # è¯»å–å®Œæ•´æ—¥å¿—ä½œä¸ºè¾“å‡º
            with open(task_log_file, 'r', encoding='utf-8') as f:
                index_tasks[task_id]["output"] = f.read()
            
            logger.error(f"Index update failed for task {task_id}, return code: {return_code}")
            
    except subprocess.TimeoutExpired:
        if task_log_file:
            log_to_file("âŒ ç´¢å¼•æ›´æ–°è¶…æ—¶ï¼ˆè¶…è¿‡1å°æ—¶ï¼‰")
        index_tasks[task_id]["status"] = "failed"
        index_tasks[task_id]["message"] = "ç´¢å¼•æ›´æ–°è¶…æ—¶ï¼ˆè¶…è¿‡1å°æ—¶ï¼‰"
        logger.error(f"Index update timeout for task {task_id}")
        
        if task_log_file and os.path.exists(task_log_file):
            with open(task_log_file, 'r', encoding='utf-8') as f:
                index_tasks[task_id]["output"] = f.read()
                
    except Exception as e:
        if task_log_file:
            log_to_file(f"âŒ ç´¢å¼•æ›´æ–°å‡ºé”™: {str(e)}")
            log_to_file(f"é”™è¯¯è¯¦æƒ…: {repr(e)}")
        index_tasks[task_id]["status"] = "failed"
        index_tasks[task_id]["message"] = f"ç´¢å¼•æ›´æ–°å‡ºé”™: {str(e)}"
        logger.error(f"Index update error for task {task_id}: {str(e)}", exc_info=True)
        
        if task_log_file and os.path.exists(task_log_file):
            with open(task_log_file, 'r', encoding='utf-8') as f:
                index_tasks[task_id]["output"] = f.read()
                
    finally:
        if task_log_file and task_logger:
            log_to_file("=" * 80)
            log_to_file(f"ä»»åŠ¡ç»“æŸï¼Œæœ€ç»ˆçŠ¶æ€: {index_tasks[task_id]['status']}")
            log_to_file("=" * 80)
            
            # å…³é—­ logger handlers
            for handler in task_logger.handlers[:]:
                handler.close()
                task_logger.removeHandler(handler)

# ========================================
# Index Update Endpoints
# ========================================

@app.post("/api/upload", response_model=IndexUpdateResponse)
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    access_key: Optional[str] = Form(None)
):
    """
    ä¸Šä¼ æ–‡ä»¶å¹¶æ›´æ–°ç´¢å¼•
    æ”¯æŒ .txt å’Œ .pdf æ–‡ä»¶
    ä½¿ç”¨é™æ€é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ï¼ˆsettings.yaml æˆ– settings_pdf.yamlï¼‰
    
    Parameters:
    - file: ä¸Šä¼ çš„æ–‡ä»¶
    - access_key: è®¿é—®å¯†é’¥ï¼ˆéœ€è¦ 'duping' æ‰èƒ½æ›´æ–°ç´¢å¼•ï¼‰
    """
    # éªŒè¯æ›´æ–°æƒé™
    if not verify_update_access(access_key):
        logger.warning(f"Unauthorized upload attempt with access_key: {access_key}")
        raise HTTPException(
            status_code=403,
            detail="è®¿é—®è¢«æ‹’ç»ï¼šæ— æ•ˆçš„è®¿é—®å¯†é’¥ã€‚ç´¢å¼•æ›´æ–°éœ€è¦æœ‰æ•ˆçš„ access_key (duping)"
        )
    
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in [".txt", ".pdf"]:
            raise HTTPException(
                status_code=400,
                detail="åªæ”¯æŒ .txt å’Œ .pdf æ–‡ä»¶"
            )
        
        file_type = file_ext[1:]  # å»æ‰ç‚¹å·
        
        # ç¡®å®šè¾“å…¥ç›®å½•
        input_dir = os.path.join(PROJECT_DIR, DATA_DIR_NAME, "input")
        os.makedirs(input_dir, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆä¿ç•™åŸå§‹æ‰©å±•åï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{file.filename}"
        file_path = os.path.join(input_dir, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        logger.info(f"File uploaded: {file_path}")
        
        # åˆ›å»ºä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        index_tasks[task_id] = {
            "status": "pending",
            "message": "æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…å¤„ç†...",
            "file_name": file.filename,
            "file_type": file_type,
            "file_path": file_path,
            "created_at": datetime.now().isoformat()
        }
        
        # åœ¨åå°è¿è¡Œç´¢å¼•æ›´æ–°
        background_tasks.add_task(
            run_index_update, 
            file_path, 
            file_type, 
            task_id
        )
        
        return IndexUpdateResponse(
            status="accepted",
            message="æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç´¢å¼•æ›´æ–°å·²å¼€å§‹",
            task_id=task_id,
            file_name=file.filename,
            file_type=file_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.get("/api/index/status/{task_id}")
async def get_index_status(task_id: str):
    """
    æŸ¥è¯¢ç´¢å¼•æ›´æ–°ä»»åŠ¡çŠ¶æ€
    """
    if task_id not in index_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return index_tasks[task_id]

@app.get("/api/index/tasks")
async def list_index_tasks():
    """
    åˆ—å‡ºæ‰€æœ‰ç´¢å¼•æ›´æ–°ä»»åŠ¡
    """
    return {
        "tasks": [
            {
                "task_id": task_id,
                **task_info
            }
            for task_id, task_info in index_tasks.items()
        ]
    }

@app.get("/api/index/logs/{task_id}")
async def get_index_logs(task_id: str, lines: int = 50):
    """
    è·å–ç´¢å¼•ä»»åŠ¡çš„å®æ—¶æ—¥å¿—
    
    Parameters:
    - task_id: ä»»åŠ¡ID
    - lines: è¿”å›æœ€åNè¡Œæ—¥å¿—ï¼ˆé»˜è®¤50è¡Œï¼‰
    """
    if task_id not in index_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    try:
        # ä¼˜å…ˆä½¿ç”¨ä»»åŠ¡ä¸“å±æ—¥å¿—æ–‡ä»¶
        task_log_file = index_tasks[task_id].get("log_file")
        log_dir = os.path.join(PROJECT_DIR, DATA_DIR_NAME, "logs")
        
        # å¦‚æœæ²¡æœ‰ä¸“å±æ—¥å¿—æ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾
        if not task_log_file or not os.path.exists(task_log_file):
            task_log_file = os.path.join(log_dir, f"task_{task_id}.log")
        
        # å¦‚æœä¸“å±æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„é€šç”¨æ—¥å¿—
        if not os.path.exists(task_log_file):
            log_files = []
            if os.path.exists(log_dir):
                for file in os.listdir(log_dir):
                    if file.endswith('.log') and not file.startswith('task_'):
                        file_path = os.path.join(log_dir, file)
                        log_files.append((file_path, os.path.getmtime(file_path)))
            
            if not log_files:
                return {
                    "task_id": task_id,
                    "logs": ["ç­‰å¾…ä»»åŠ¡å¼€å§‹..."],
                    "current_workflow": "ç­‰å¾…ä¸­",
                    "progress": 0,
                    "log_file": "æ— "
                }
            
            # è·å–æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
            task_log_file = sorted(log_files, key=lambda x: x[1], reverse=True)[0][0]
        
        latest_log = task_log_file
        
        # è¯»å–æœ€åNè¡Œ
        with open(latest_log, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        # è§£æå½“å‰å·¥ä½œæµå’Œè¿›åº¦
        current_workflow = "åˆå§‹åŒ–"
        progress = 0
        
        # GraphRAG å·¥ä½œæµå…³é”®è¯
        workflows = [
            ("create_base_text_units", "åˆ›å»ºæ–‡æœ¬å•å…ƒ", 10),
            ("create_base_extracted_entities", "æå–å®ä½“", 20),
            ("create_summarized_entities", "æ€»ç»“å®ä½“", 30),
            ("create_base_entity_graph", "æ„å»ºå®ä½“å›¾", 40),
            ("create_final_entities", "ç”Ÿæˆæœ€ç»ˆå®ä½“", 50),
            ("create_final_relationships", "ç”Ÿæˆå…³ç³»", 60),
            ("create_final_communities", "åˆ›å»ºç¤¾åŒº", 70),
            ("create_final_community_reports", "ç”Ÿæˆç¤¾åŒºæŠ¥å‘Š", 80),
            ("create_final_text_units", "ç”Ÿæˆæœ€ç»ˆæ–‡æœ¬å•å…ƒ", 90),
            ("create_final_documents", "ç”Ÿæˆæ–‡æ¡£", 95),
        ]
        
        # ä»æ—¥å¿—ä¸­æŸ¥æ‰¾å½“å‰å·¥ä½œæµ
        for line in reversed(recent_lines):
            for workflow_key, workflow_name, workflow_progress in workflows:
                if workflow_key in line:
                    current_workflow = workflow_name
                    progress = workflow_progress
                    break
            if progress > 0:
                break
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if any("completed" in line.lower() or "success" in line.lower() for line in recent_lines[-5:]):
            current_workflow = "ç´¢å¼•æ„å»ºå®Œæˆ"
            progress = 100
        
        return {
            "task_id": task_id,
            "logs": [line.strip() for line in recent_lines],
            "current_workflow": current_workflow,
            "progress": progress,
            "log_file": os.path.basename(latest_log)
        }
        
    except Exception as e:
        logger.error(f"Failed to read logs for task {task_id}: {str(e)}")
        return {
            "task_id": task_id,
            "logs": [],
            "current_workflow": f"è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}",
            "progress": 0
        }

# ========================================
# Main
# ========================================

if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    
    logger.info(f"Starting server at http://{host}:{port}")
    logger.info("Note: Access via Nginx at http://localhost (port 80)")
    uvicorn.run("graphrag_service:app", host=host, port=port, reload=True)
