#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°† GraphRAG ç”Ÿæˆçš„ parquet æ–‡ä»¶å¯¼å…¥åˆ° Neo4j æ•°æ®åº“
é€‚ç”¨äº macOS ç³»ç»Ÿ
"""

import pandas as pd
from neo4j import GraphDatabase
from pathlib import Path
import logging
from tqdm import tqdm
from typing import List, Dict, Any
import warnings

warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ========================================
# Neo4j è¿æ¥é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„ Neo4j è¿æ¥ä¿¡æ¯
# ========================================
NEO4J_URI = "bolt://localhost:7687"  # Neo4j è¿æ¥åœ°å€ï¼Œé»˜è®¤ç«¯å£ 7687
NEO4J_USERNAME = "neo4j"              # Neo4j ç”¨æˆ·åï¼Œé»˜è®¤ä¸º neo4j
NEO4J_PASSWORD = "Han9510!"      # âš ï¸ ä¿®æ”¹ä¸ºä½ è®¾ç½®çš„å¯†ç 
NEO4J_DATABASE = "neo4j"              # æ•°æ®åº“åç§°ï¼Œç¤¾åŒºç‰ˆåªèƒ½ä½¿ç”¨ neo4j

# ========================================
# Parquet æ–‡ä»¶è·¯å¾„é…ç½® - ä¿®æ”¹ä¸ºä½ çš„ output ç›®å½•è·¯å¾„
# ========================================
# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
SCRIPT_DIR = Path(__file__).parent.parent
OUTPUT_DIR = SCRIPT_DIR / "data" / "output"  # GraphRAG ç”Ÿæˆçš„ parquet æ–‡ä»¶æ‰€åœ¨ç›®å½•

# Parquet æ–‡ä»¶è·¯å¾„
DOCUMENTS_PATH = OUTPUT_DIR / "documents.parquet"
TEXT_UNITS_PATH = OUTPUT_DIR / "text_units.parquet"
ENTITIES_PATH = OUTPUT_DIR / "entities.parquet"
RELATIONSHIPS_PATH = OUTPUT_DIR / "relationships.parquet"
COMMUNITIES_PATH = OUTPUT_DIR / "communities.parquet"
COMMUNITY_REPORTS_PATH = OUTPUT_DIR / "community_reports.parquet"


class Neo4jImporter:
    """Neo4j æ•°æ®å¯¼å…¥å™¨ - ä½¿ç”¨å¹¶è¡Œæ‰¹é‡å¯¼å…¥æé«˜æ€§èƒ½"""
    
    def __init__(self, uri: str, username: str, password: str, database: str = "neo4j"):
        """
        åˆå§‹åŒ– Neo4j è¿æ¥
        
        å‚æ•°:
            uri: Neo4j è¿æ¥åœ°å€
            username: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“åç§°
        """
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.database = database
        logger.info(f"âœ… å·²è¿æ¥åˆ° Neo4j: {uri}")
    
    def close(self):
        """å…³é—­ Neo4j è¿æ¥"""
        self.driver.close()
        logger.info("Neo4j è¿æ¥å·²å…³é—­")
    
    def clear_database(self):
        """
        æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
        âš ï¸ è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯é€†ï¼Œä¼šåˆ é™¤æ‰€æœ‰æ•°æ®
        """
        with self.driver.session(database=self.database) as session:
            session.run("MATCH (n) DETACH DELETE n")
            logger.info("âœ… æ•°æ®åº“å·²æ¸…ç©º")
    
    def create_constraints(self):
        """
        åˆ›å»ºçº¦æŸå’Œç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        - ä¸ºå®ä½“ ID åˆ›å»ºå”¯ä¸€æ€§çº¦æŸ
        - ä¸ºå®ä½“åç§°åˆ›å»ºç´¢å¼•
        - ä¸ºç¤¾åŒºã€æ–‡æ¡£ã€æ–‡æœ¬å•å…ƒåˆ›å»ºçº¦æŸ
        """
        with self.driver.session(database=self.database) as session:
            constraints = [
                ("graphrag_entity_id", "FOR (e:__Entity__) REQUIRE e.id IS UNIQUE", "__Entity__ ID"),
                ("graphrag_relationship_id", "FOR (r:__Relationship__) REQUIRE r.id IS UNIQUE", "__Relationship__ ID"),
                ("graphrag_community_id", "FOR (c:__Community__) REQUIRE c.id IS UNIQUE", "__Community__ ID"),
                ("graphrag_document_id", "FOR (d:__Document__) REQUIRE d.id IS UNIQUE", "__Document__ ID"),
                ("graphrag_text_unit_id", "FOR (t:__Chunk__) REQUIRE t.id IS UNIQUE", "__Chunk__ ID"),
            ]
            
            for constraint_name, constraint_def, description in constraints:
                try:
                    session.run(f"CREATE CONSTRAINT {constraint_name} IF NOT EXISTS {constraint_def}")
                    logger.info(f"âœ… å·²åˆ›å»º {description} å”¯ä¸€æ€§çº¦æŸ")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»º {description} çº¦æŸå¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
            
            # åˆ›å»ºç´¢å¼•
            indexes = [
                ("graphrag_entity_name", "FOR (e:__Entity__) ON (e.name)", "__Entity__ name"),
                ("graphrag_entity_human_readable_id", "FOR (e:__Entity__) ON (e.human_readable_id)", "__Entity__ human_readable_id"),
                ("graphrag_relationship_human_readable_id", "FOR (r:__Relationship__) ON (r.human_readable_id)", "__Relationship__ human_readable_id"),
                ("graphrag_community_title", "FOR (c:__Community__) ON (c.title)", "__Community__ title"),
            ]
            
            for index_name, index_def, description in indexes:
                try:
                    session.run(f"CREATE INDEX {index_name} IF NOT EXISTS {index_def}")
                    logger.info(f"âœ… å·²åˆ›å»º {description} ç´¢å¼•")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»º {description} ç´¢å¼•å¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
    
    def parallel_batched_import(self, statement: str, df: pd.DataFrame, 
                               batch_size: int = 100, max_workers: int = 1):
        """
        ä½¿ç”¨å¹¶è¡Œå¤„ç†è¿›è¡Œæ‰¹é‡å¯¼å…¥æ•°æ®åˆ° Neo4jï¼ˆå‚è€ƒ Notebook ä¸­çš„å®ç°ï¼‰
        
        å‚æ•°:
            statement: Cypher æŸ¥è¯¢è¯­å¥ï¼Œä½¿ç”¨ value ä½œä¸ºæ¯è¡Œæ•°æ®çš„å¼•ç”¨
            df: è¦å¯¼å…¥çš„ DataFrame
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°ï¼Œé»˜è®¤ 100
            max_workers: å¹¶è¡Œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ 8
        
        è¿”å›:
            å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        import time
        import concurrent.futures
        
        # 1. åˆå§‹åŒ–ï¼Œè®¡ç®—æ€»è¡Œæ•°ã€æ‰¹æ¬¡æ•°ï¼Œå¹¶è®°å½•å¼€å§‹æ—¶é—´
        total = len(df)
        batches = (total + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
        start_time = time.time()
        results = []
        
        logger.info(f"å¼€å§‹å¹¶è¡Œå¯¼å…¥ {total} è¡Œæ•°æ®ï¼Œåˆ†ä¸º {batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} æ¡")
        
        # 2. å®šä¹‰æ‰¹å¤„ç†å‡½æ•°
        def process_batch(batch_idx):
            """
            æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®
            
            å‚æ•°:
                batch_idx: æ‰¹æ¬¡ç´¢å¼•
            
            è¿”å›:
                æ‰¹æ¬¡å¤„ç†ç»“æœå­—å…¸
            """
            # è®¡ç®—æ‰¹æ¬¡çš„èµ·å§‹å’Œç»“æŸç´¢å¼•
            start = batch_idx * batch_size
            end = min(start + batch_size, total)
            batch = df.iloc[start:end]
            
            batch_start_time = time.time()
            
            try:
                with self.driver.session(database=self.database) as session:
                    # UNWIND æ˜¯ Cypher ä¸­çš„å…³é”®å­—ï¼Œç”¨äºå°†åˆ—è¡¨å±•å¼€ä¸ºå¤šè¡Œ
                    # $rows æ˜¯å‚æ•°ï¼Œè¡¨ç¤ºå°†è¦ä¼ å…¥çš„è¡Œæ•°æ®
                    # å®Œæ•´æ„æ€ï¼šå°† $rows å‚æ•°ï¼ˆä¸€ä¸ªåˆ—è¡¨ï¼‰ä¸­çš„æ¯ä¸ªå…ƒç´ å±•å¼€ï¼Œ
                    # æ¯ä¸ªå…ƒç´ è¢«èµ‹å€¼ç»™å˜é‡ valueï¼Œå¯¹æ¯ä¸ª value æ‰§è¡Œåç»­çš„ Cypher è¯­å¥
                    result = session.run(
                        "UNWIND $rows AS value " + statement,
                        rows=batch.to_dict("records")  # å°† DataFrame è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    )
                    summary = result.consume()  # è·å–æŸ¥è¯¢çš„æ‘˜è¦ä¿¡æ¯ï¼ˆæ‰§è¡Œç»Ÿè®¡ï¼‰
                    batch_duration = time.time() - batch_start_time
                    
                    return {
                        "batch": batch_idx,
                        "rows": end - start,
                        "success": True,
                        "duration": batch_duration,
                        "counters": summary.counters  # ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ›å»º/æ›´æ–°çš„èŠ‚ç‚¹æ•°ç­‰ï¼‰
                    }
            except Exception as e:
                batch_duration = time.time() - batch_start_time
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx} (è¡Œ {start}-{end-1}) å¤„ç†å¤±è´¥: {str(e)}")
                
                return {
                    "batch": batch_idx,
                    "rows": end - start,
                    "success": False,
                    "duration": batch_duration,
                    "error": str(e)
                }
        
        # 3. ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            futures = [executor.submit(process_batch, i) for i in range(batches)]
            
            # å¤„ç†å®Œæˆçš„æ‰¹æ¬¡
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                result = future.result()
                results.append(result)
                
                if result["success"]:
                    logger.info(f"âœ… æ‰¹æ¬¡ {result['batch']} å®Œæˆ: {result['rows']} è¡Œ, "
                              f"è€—æ—¶ {result['duration']:.2f}ç§’")
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡ {result['batch']} å¤±è´¥: {result['rows']} è¡Œ, "
                               f"è€—æ—¶ {result['duration']:.2f}ç§’")
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (i + 1) / batches * 100
                logger.info(f"ğŸ“Š è¿›åº¦: {i+1}/{batches} æ‰¹æ¬¡å®Œæˆ ({progress:.1f}%)")
        
        # 4. ç»Ÿè®¡ç»“æœ
        successful_rows = sum(r["rows"] for r in results if r["success"])
        failed_rows = sum(r["rows"] for r in results if not r["success"])
        
        duration = time.time() - start_time
        rows_per_second = successful_rows / duration if duration > 0 else 0
        
        logger.info(f"âœ… å¯¼å…¥å®Œæˆ: æ€»è®¡ {total} è¡Œ, æˆåŠŸ {successful_rows} è¡Œ, å¤±è´¥ {failed_rows} è¡Œ")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f}ç§’, å¹³å‡é€Ÿåº¦: {rows_per_second:.2f} è¡Œ/ç§’")
        
        return {
            "total_rows": total,
            "successful_rows": successful_rows,
            "failed_rows": failed_rows,
            "duration_seconds": duration,
            "rows_per_second": rows_per_second,
            "batch_results": results
        }

    def import_entities(self, entities_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥å®ä½“èŠ‚ç‚¹åˆ° Neo4j
        
        å‚æ•°:
            entities_df: å®ä½“ DataFrameï¼ˆä» entities.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ“¦ å¼€å§‹å¯¼å…¥ {len(entities_df)} ä¸ªå®ä½“...")
        
        # æ£€æŸ¥å®é™…çš„åˆ—åï¼ŒGraphRAG å¯èƒ½ä½¿ç”¨ 'title' è€Œä¸æ˜¯ 'name'
        id_column = 'title' if 'title' in entities_df.columns else 'name' if 'name' in entities_df.columns else 'id'
        logger.info(f"ä½¿ç”¨ '{id_column}' åˆ—ä½œä¸ºå®ä½“ ID")
        
        # æ£€æŸ¥ human_readable_id
        if 'human_readable_id' in entities_df.columns:
            logger.info(f"âœ… human_readable_id èŒƒå›´: {entities_df['human_readable_id'].min()} - {entities_df['human_readable_id'].max()}")
        else:
            logger.warning("âš ï¸  æœªæ‰¾åˆ° human_readable_id åˆ—")
        
        # ä¸º ID ä¸º null çš„å®ä½“ç”Ÿæˆå”¯ä¸€ ID
        null_count = entities_df[id_column].isna().sum()
        if null_count > 0:
            logger.warning(f"âš ï¸  å‘ç° {null_count} ä¸ª {id_column} ä¸º null çš„å®ä½“ï¼Œå°†ç”Ÿæˆå”¯ä¸€ ID")
            entities_df[id_column] = entities_df.apply(
                lambda row: f"__NULL_ENTITY_{row['human_readable_id']}" if pd.isna(row[id_column]) else row[id_column],
                axis=1
            )
        
        # Cypher è¯­å¥ï¼šç›´æ¥ä½¿ç”¨ human_readable_idï¼Œä¸åˆ›å»º graphrag_id
        statement = f"""
        MERGE (e:__Entity__ {{id: value.{id_column}}})
        SET e.name = value.{id_column},
            e.type = value.type,
            e.description = value.description,
            e.human_readable_id = value.human_readable_id,
            e.text_unit_ids = value.text_unit_ids
        """
        
        # ä½¿ç”¨å¹¶è¡Œæ‰¹é‡å¯¼å…¥
        result = self.parallel_batched_import(statement, entities_df, batch_size=batch_size)
        logger.info(f"âœ… å®ä½“å¯¼å…¥å®Œæˆ: {result['successful_rows']}/{result['total_rows']} æˆåŠŸ")
        return result
    
    def import_relationships(self, relationships_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥å…³ç³»åˆ° Neo4jï¼šåˆ›å»º __Relationship__ èŠ‚ç‚¹å’Œ RELATED_TO è¾¹
        
        å‚æ•°:
            relationships_df: å…³ç³» DataFrameï¼ˆä» relationships.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ”— å¼€å§‹å¯¼å…¥ {len(relationships_df)} ä¸ªå…³ç³»...")
        
        # æ£€æŸ¥ human_readable_id
        if 'human_readable_id' in relationships_df.columns:
            logger.info(f"âœ… å…³ç³» human_readable_id èŒƒå›´: {relationships_df['human_readable_id'].min()} - {relationships_df['human_readable_id'].max()}")
        
        # æ­¥éª¤ 1: åˆ›å»º __Relationship__ å…ƒæ•°æ®èŠ‚ç‚¹
        relationship_node_statement = """
        MERGE (r:__Relationship__ {id: value.id})
        SET r.human_readable_id = value.human_readable_id,
            r.source = value.source,
            r.target = value.target,
            r.description = value.description,
            r.weight = value.weight,
            r.text_unit_ids = value.text_unit_ids
        """
        
        logger.info("ğŸ“¦ åˆ›å»º __Relationship__ å…ƒæ•°æ®èŠ‚ç‚¹...")
        result_nodes = self.parallel_batched_import(relationship_node_statement, relationships_df, batch_size=batch_size)
        logger.info(f"âœ… __Relationship__ èŠ‚ç‚¹åˆ›å»ºå®Œæˆ: {result_nodes['successful_rows']}/{result_nodes['total_rows']} æˆåŠŸ")
        
        # æ­¥éª¤ 2: åˆ›å»ºå®ä½“ä¹‹é—´çš„ RELATED_TO è¾¹
        relationship_edge_statement = """
        MATCH (source:__Entity__ {id: value.source})
        MATCH (target:__Entity__ {id: value.target})
        MERGE (source)-[r:RELATED_TO]->(target)
        SET r.description = value.description,
            r.weight = value.weight,
            r.relationship_id = value.id
        """
        
        logger.info("ğŸ”— åˆ›å»ºå®ä½“ä¹‹é—´çš„ RELATED_TO è¾¹...")
        result_edges = self.parallel_batched_import(relationship_edge_statement, relationships_df, batch_size=batch_size)
        logger.info(f"âœ… RELATED_TO è¾¹åˆ›å»ºå®Œæˆ: {result_edges['successful_rows']}/{result_edges['total_rows']} æˆåŠŸ")
        
        return {
            'nodes': result_nodes,
            'edges': result_edges,
            'total_rows': len(relationships_df),
            'successful_rows': min(result_nodes['successful_rows'], result_edges['successful_rows'])
        }
    
    def import_documents(self, documents_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥æ–‡æ¡£èŠ‚ç‚¹åˆ° Neo4j
        
        å‚æ•°:
            documents_df: æ–‡æ¡£ DataFrameï¼ˆä» documents.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ“„ å¼€å§‹å¯¼å…¥ {len(documents_df)} ä¸ªæ–‡æ¡£...")
        
        statement = """
        MERGE (d:__Document__ {id: value.id})
        SET d.title = value.title,
            d.raw_content = value.raw_content,
            d.text_unit_ids = value.text_unit_ids
        """
        
        result = self.parallel_batched_import(statement, documents_df, batch_size=batch_size)
        logger.info(f"âœ… æ–‡æ¡£å¯¼å…¥å®Œæˆ: {result['successful_rows']}/{result['total_rows']} æˆåŠŸ")
        return result
    
    def import_text_units(self, text_units_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥æ–‡æœ¬å•å…ƒèŠ‚ç‚¹åˆ° Neo4jï¼Œå¹¶å…³è”åˆ°æ–‡æ¡£ã€å®ä½“å’Œå…³ç³»
        
        å‚æ•°:
            text_units_df: æ–‡æœ¬å•å…ƒ DataFrameï¼ˆä» text_units.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ“ å¼€å§‹å¯¼å…¥ {len(text_units_df)} ä¸ªæ–‡æœ¬å•å…ƒ...")
        
        # æ­¥éª¤ 1: åˆ›å»ºæ–‡æœ¬å—èŠ‚ç‚¹å¹¶è¿æ¥åˆ°æ–‡æ¡£
        chunk_statement = """
        MERGE (t:__Chunk__ {id: value.id})
        SET t.text = value.text,
            t.n_tokens = value.n_tokens,
            t.document_ids = value.document_ids,
            t.entity_ids = value.entity_ids,
            t.relationship_ids = value.relationship_ids
        WITH t, value
        UNWIND value.document_ids AS doc_id
        MATCH (d:__Document__ {id: doc_id})
        MERGE (t)-[:PART_OF]->(d)
        """
        
        result_chunks = self.parallel_batched_import(chunk_statement, text_units_df, batch_size=batch_size)
        logger.info(f"âœ… æ–‡æœ¬å•å…ƒèŠ‚ç‚¹åˆ›å»ºå®Œæˆ: {result_chunks['successful_rows']}/{result_chunks['total_rows']} æˆåŠŸ")
        
        # æ­¥éª¤ 2: è¿æ¥æ–‡æœ¬å—åˆ°å®ä½“
        logger.info("ğŸ”— è¿æ¥æ–‡æœ¬å—åˆ°å®ä½“...")
        entity_link_statement = """
        MATCH (t:__Chunk__ {id: value.id})
        WITH t, value.entity_ids AS entity_ids
        WHERE entity_ids IS NOT NULL AND size(entity_ids) > 0
        UNWIND entity_ids AS entity_id
        MATCH (e:__Entity__ {id: entity_id})
        MERGE (t)-[:MENTIONS]->(e)
        """
        
        result_entities = self.parallel_batched_import(entity_link_statement, text_units_df, batch_size=batch_size)
        logger.info(f"âœ… æ–‡æœ¬å—-å®ä½“è¿æ¥å®Œæˆ: {result_entities['successful_rows']}/{result_entities['total_rows']} æˆåŠŸ")
        
        # æ­¥éª¤ 3: è¿æ¥æ–‡æœ¬å—åˆ°å…³ç³»èŠ‚ç‚¹
        logger.info("ğŸ”— è¿æ¥æ–‡æœ¬å—åˆ°å…³ç³»...")
        relationship_link_statement = """
        MATCH (t:__Chunk__ {id: value.id})
        WITH t, value.relationship_ids AS relationship_ids
        WHERE relationship_ids IS NOT NULL AND size(relationship_ids) > 0
        UNWIND relationship_ids AS rel_id
        MATCH (r:__Relationship__ {id: rel_id})
        MERGE (t)-[:HAS_RELATIONSHIP]->(r)
        """
        
        result_relationships = self.parallel_batched_import(relationship_link_statement, text_units_df, batch_size=batch_size)
        logger.info(f"âœ… æ–‡æœ¬å—-å…³ç³»è¿æ¥å®Œæˆ: {result_relationships['successful_rows']}/{result_relationships['total_rows']} æˆåŠŸ")
        
        return {
            'chunks': result_chunks,
            'entity_links': result_entities,
            'relationship_links': result_relationships,
            'total_rows': len(text_units_df),
            'successful_rows': result_chunks['successful_rows'],
            'duration_seconds': result_chunks['duration_seconds'] + result_entities['duration_seconds'] + result_relationships['duration_seconds']
        }
    
    def import_communities(self, communities_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥ç¤¾åŒºèŠ‚ç‚¹åˆ° Neo4jï¼Œå¹¶å…³è”å®ä½“
        
        å‚æ•°:
            communities_df: ç¤¾åŒº DataFrameï¼ˆä» communities.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ˜ï¸ å¼€å§‹å¯¼å…¥ {len(communities_df)} ä¸ªç¤¾åŒº...")
        
        statement = """
        MERGE (c:__Community__ {id: value.id})
        SET c.title = value.title,
            c.level = value.level,
            c.entity_ids = value.entity_ids,
            c.relationship_ids = value.relationship_ids,
            c.text_unit_ids = value.text_unit_ids
        WITH c, value
        UNWIND value.entity_ids AS entity_id
        MATCH (e:__Entity__ {id: entity_id})
        MERGE (e)-[:BELONGS_TO]->(c)
        """
        
        result = self.parallel_batched_import(statement, communities_df, batch_size=batch_size)
        logger.info(f"âœ… ç¤¾åŒºå¯¼å…¥å®Œæˆ: {result['successful_rows']}/{result['total_rows']} æˆåŠŸ")
        return result
    
    def import_community_reports(self, reports_df: pd.DataFrame, batch_size: int = 100):
        """
        å¯¼å…¥ç¤¾åŒºæŠ¥å‘Šï¼Œå¹¶å…³è”åˆ°ç¤¾åŒº
        
        å‚æ•°:
            reports_df: ç¤¾åŒºæŠ¥å‘Š DataFrameï¼ˆä» community_reports.parquet è¯»å–ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°
        """
        logger.info(f"ğŸ“Š å¼€å§‹å¯¼å…¥ {len(reports_df)} ä¸ªç¤¾åŒºæŠ¥å‘Š...")
        
        statement = """
        MATCH (c:__Community__ {id: value.community})
        SET c.summary = value.summary,
            c.full_content = value.full_content,
            c.rank = value.rank,
            c.rank_explanation = value.rank_explanation,
            c.findings = value.findings
        """
        
        result = self.parallel_batched_import(statement, reports_df, batch_size=batch_size)
        logger.info(f"âœ… ç¤¾åŒºæŠ¥å‘Šå¯¼å…¥å®Œæˆ: {result['successful_rows']}/{result['total_rows']} æˆåŠŸ")
        return result


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„ GraphRAG æ•°æ®å¯¼å…¥æµç¨‹
    
    æµç¨‹ï¼š
    1. æ£€æŸ¥ parquet æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. è¯»å–æ‰€æœ‰æ•°æ®
    3. è¿æ¥ Neo4j æ•°æ®åº“
    4. åˆ›å»ºçº¦æŸå’Œç´¢å¼•
    5. å¹¶è¡Œæ‰¹é‡å¯¼å…¥æ‰€æœ‰æ•°æ®
    6. æ˜¾ç¤ºå¯¼å…¥ç»“æœ
    """
    logger.info("=" * 70)
    logger.info("ğŸš€ å¼€å§‹å¯¼å…¥ GraphRAG æ•°æ®åˆ° Neo4j")
    logger.info("=" * 70)
    
    # 1. æ£€æŸ¥å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = {
        "å®ä½“": ENTITIES_PATH,
        "å…³ç³»": RELATIONSHIPS_PATH,
    }
    
    for name, path in required_files.items():
        if not path.exists():
            logger.error(f"âŒ {name}æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            logger.error("è¯·å…ˆè¿è¡Œ GraphRAG ç´¢å¼•æ„å»ºç”Ÿæˆ parquet æ–‡ä»¶")
            return
    
    # 2. è¯»å– parquet æ–‡ä»¶
    logger.info("ğŸ“– è¯»å– parquet æ–‡ä»¶...")
    data_files = {}
    
    try:
        # å¿…éœ€æ–‡ä»¶
        data_files['entities'] = pd.read_parquet(ENTITIES_PATH)
        data_files['relationships'] = pd.read_parquet(RELATIONSHIPS_PATH)
        
        logger.info(f"ğŸ“Š å®ä½“æ•°é‡: {len(data_files['entities'])}")
        logger.info(f"ğŸ“Š å…³ç³»æ•°é‡: {len(data_files['relationships'])}")
        
        # å¯é€‰æ–‡ä»¶
        optional_files = {
            'documents': DOCUMENTS_PATH,
            'text_units': TEXT_UNITS_PATH,
            'communities': COMMUNITIES_PATH,
            'community_reports': COMMUNITY_REPORTS_PATH,
        }
        
        for key, path in optional_files.items():
            if path.exists():
                data_files[key] = pd.read_parquet(path)
                logger.info(f"ğŸ“Š {key} æ•°é‡: {len(data_files[key])}")
            else:
                logger.warning(f"âš ï¸  {key} æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
        
        # æ˜¾ç¤ºå®ä½“ç±»å‹åˆ†å¸ƒ
        if 'type' in data_files['entities'].columns:
            type_counts = data_files['entities']['type'].value_counts()
            logger.info(f"ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ: {dict(type_counts.head(5))}")
        
    except Exception as e:
        logger.error(f"âŒ è¯»å– parquet æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return
    
    # 3. è¿æ¥ Neo4j å¹¶å¯¼å…¥
    logger.info(f"ğŸ”Œ è¿æ¥ Neo4j: {NEO4J_URI}")
    importer = Neo4jImporter(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, NEO4J_DATABASE)
    
    results = {}
    total_duration = 0
    
    try:
        # æ¸…ç©ºæ•°æ®åº“
        logger.info("âš ï¸  æ¸…ç©ºæ•°æ®åº“...")
        importer.clear_database()
        
        # 4. åˆ›å»ºçº¦æŸå’Œç´¢å¼•
        logger.info("ğŸ”§ åˆ›å»ºçº¦æŸå’Œç´¢å¼•...")
        importer.create_constraints()
        
        # 5. å¯¼å…¥æ•°æ®ï¼ˆæŒ‰ä¾èµ–é¡ºåºï¼‰
        step = 1
        
        # æ­¥éª¤ 1: å¯¼å…¥æ–‡æ¡£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'documents' in data_files:
            logger.info("\n" + "=" * 70)
            logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥æ–‡æ¡£")
            logger.info("=" * 70)
            results['documents'] = importer.import_documents(data_files['documents'], batch_size=100)
            total_duration += results['documents']['duration_seconds']
            step += 1
        
        # æ­¥éª¤ 2: å¯¼å…¥å®ä½“
        logger.info("\n" + "=" * 70)
        logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥å®ä½“èŠ‚ç‚¹")
        logger.info("=" * 70)
        results['entities'] = importer.import_entities(data_files['entities'], batch_size=100)
        total_duration += results['entities']['duration_seconds']
        step += 1
        
        # æ­¥éª¤ 3: å¯¼å…¥å…³ç³»
        logger.info("\n" + "=" * 70)
        logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥å…³ç³»")
        logger.info("=" * 70)
        results['relationships'] = importer.import_relationships(data_files['relationships'], batch_size=100)
        # å…³ç³»å¯¼å…¥è¿”å›çš„æ˜¯åŒ…å« nodes å’Œ edges çš„å­—å…¸
        if 'nodes' in results['relationships']:
            total_duration += results['relationships']['nodes']['duration_seconds']
            total_duration += results['relationships']['edges']['duration_seconds']
        step += 1
        
        # æ­¥éª¤ 4: å¯¼å…¥æ–‡æœ¬å•å…ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'text_units' in data_files:
            logger.info("\n" + "=" * 70)
            logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥æ–‡æœ¬å•å…ƒ")
            logger.info("=" * 70)
            results['text_units'] = importer.import_text_units(data_files['text_units'], batch_size=100)
            if 'duration_seconds' in results['text_units']:
                total_duration += results['text_units']['duration_seconds']
            step += 1
        
        # æ­¥éª¤ 5: å¯¼å…¥ç¤¾åŒºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'communities' in data_files:
            logger.info("\n" + "=" * 70)
            logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥ç¤¾åŒº")
            logger.info("=" * 70)
            results['communities'] = importer.import_communities(data_files['communities'], batch_size=100)
            total_duration += results['communities']['duration_seconds']
            step += 1
        
        # æ­¥éª¤ 6: å¯¼å…¥ç¤¾åŒºæŠ¥å‘Šï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'community_reports' in data_files:
            logger.info("\n" + "=" * 70)
            logger.info(f"ç¬¬ {step} æ­¥ï¼šå¯¼å…¥ç¤¾åŒºæŠ¥å‘Š")
            logger.info("=" * 70)
            results['community_reports'] = importer.import_community_reports(data_files['community_reports'], batch_size=100)
            total_duration += results['community_reports']['duration_seconds']
            step += 1
        
        # 6. æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        logger.info("\n" + "=" * 70)
        logger.info("âœ… æ•°æ®å¯¼å…¥å®Œæˆï¼")
        logger.info("=" * 70)
        
        for key, result in results.items():
            logger.info(f"ğŸ“Š {key}: {result['successful_rows']}/{result['total_rows']} æˆåŠŸ")
        
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f} ç§’")
        logger.info("\nğŸŒ è¯·è®¿é—® http://localhost:7474 æŸ¥çœ‹çŸ¥è¯†å›¾è°±")
        logger.info("   é»˜è®¤ç”¨æˆ·å: neo4j")
        logger.info("   å¯†ç : ä½ è®¾ç½®çš„å¯†ç ")
        logger.info("\nğŸ’¡ åœ¨ Neo4j æµè§ˆå™¨ä¸­è¿è¡Œä»¥ä¸‹æŸ¥è¯¢æŸ¥çœ‹å›¾è°±:")
        logger.info("   # æŸ¥çœ‹å®ä½“")
        logger.info("   MATCH (n:__Entity__) RETURN n LIMIT 25")
        logger.info("\n   # æŸ¥çœ‹ç¤¾åŒº")
        logger.info("   MATCH (c:__Community__) RETURN c LIMIT 10")
        logger.info("\n   # æŸ¥çœ‹å®ä½“å’Œå®ƒä»¬æ‰€å±çš„ç¤¾åŒº")
        logger.info("   MATCH (e:__Entity__)-[:BELONGS_TO]->(c:__Community__) RETURN e, c LIMIT 25")
        logger.info("\n   # é€šè¿‡ human_readable_id æŸ¥è¯¢å®ä½“ï¼ˆä¾‹å¦‚æŸ¥è¯¢å®ä½“ 20397ï¼‰")
        logger.info("   MATCH (e:__Entity__ {human_readable_id: 20397}) RETURN e")
        logger.info("\n   # é€šè¿‡å®ä½“åç§°æŸ¥è¯¢")
        logger.info("   MATCH (e:__Entity__ {id: 'å±‚ææ¶²'}) RETURN e")
        logger.info("=" * 70)
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        importer.close()


if __name__ == "__main__":
    main()
